{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data():\n",
    "    with open(\"./data/train.jsonl\", 'r') as f:\n",
    "        train_data = [json.loads(line) for line in f]\n",
    "\n",
    "    tag_mapping = {1: \"NN\", 2: \"DT\", 3: \"JJ\", 4: \"OT\"}\n",
    "\n",
    "    # Preprocess training data\n",
    "    train_sentences = []\n",
    "    train_labels = []\n",
    "    for entry in train_data:\n",
    "        tokens = entry['tokens']\n",
    "        pos_tags = entry['pos_tags']\n",
    "        chunk_tags = np.array(entry['chunk_tags'])\n",
    "        \n",
    "        train_sentences.append(pos_tags)\n",
    "        train_labels.append(chunk_tags)\n",
    "\n",
    "    with open(\"./data/test.jsonl\", 'r') as f:\n",
    "        test_data = [json.loads(line) for line in f]\n",
    "    # Preprocess training data\n",
    "    test_sentences = []\n",
    "    test_labels = []\n",
    "    for entry in test_data:\n",
    "        tokens = entry['tokens']\n",
    "        pos_tags = entry['pos_tags']\n",
    "        chunk_tags = np.array(entry['chunk_tags'])\n",
    "        \n",
    "        test_sentences.append(pos_tags)\n",
    "        test_labels.append(chunk_tags)\n",
    "    \n",
    "    return train_sentences, test_sentences,  train_labels, test_labels\n",
    "\n",
    "X_train, X_test, y_train, y_test= fetch_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(input_list):\n",
    "    encoded_list = []\n",
    "    for item in input_list:\n",
    "        one_hot_vector = np.zeros(4)\n",
    "        one_hot_vector[item - 1] = 1  # Adjust index to start from 0\n",
    "        encoded_list.append(one_hot_vector.tolist())\n",
    "    return np.array(encoded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encode(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([np.array([1,0,0]), np.array([1,0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def into_ho(X_train):\n",
    "    X_train_ho = []\n",
    "    for i in range(len(X_train)):\n",
    "        X = one_hot_encode(X_train[i])\n",
    "        temp = []\n",
    "        for j in range(len(X)):\n",
    "            if j==0:\n",
    "                temp.append(np.concatenate([np.array([1.0,0.0,0.0,0.0,0.0]), X[j]]))\n",
    "            else:\n",
    "                temp.append((np.concatenate([np.array([0]), X[j-1], X[j]] )))\n",
    "        X_train_ho.append(np.array(temp))\n",
    "    return X_train_ho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ho = into_ho(X_train)\n",
    "X_test_ho = into_ho(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train[0],\"\\n\",X_train_ho[0])\n",
    "# X_train_ho[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1.0+(np.exp(-x)))\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_vals = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_vals / np.sum(exp_vals, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([np.array([0]), X_train_ho[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.9, 8.9, 8.9, 8.9, 8.9, 8.9, 8.9, 8.9, 8.9, 8.9])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8.9*np.ones(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred, epsilon=1e-15):\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    loss = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    total_loss = np.sum(loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    np.mean(y_train[18]==(predict[18]>0.5).astype(int))\n",
    "    acc =  np.mean(y_true == (y_pred>0.5 ).astype(int))\n",
    "    print(acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleRecurrentPerceptron:\n",
    "    def __init__(self, vec_len, lr):\n",
    "          \n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.random.randn(vec_len)\n",
    "        self.threshold = np.random.randn(1)\n",
    "        self.lr = lr\n",
    "\n",
    "   \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"inputs-- (B, Tx, 10)\"\"\"   \n",
    "        prediction= []    #(B, Tx)\n",
    "        X_i_b = []      #(B, Tx, 10)\n",
    "        for j in range(len(inputs)):\n",
    "            out=[]\n",
    "            X_i_j = []\n",
    "            Tx, _ = inputs[j].shape\n",
    "            y_prev=0\n",
    "            for i in range(Tx):\n",
    "                x = np.concatenate([inputs[j][i], np.array([y_prev])])\n",
    "                X_i_j.append(x)\n",
    "                net = x.T @ self.weights - self.threshold[0]\n",
    "                oi = sigmoid(net)\n",
    "                y_prev = oi\n",
    "                out.append(oi)\n",
    "            prediction.append(np.array(out))\n",
    "            X_i_b.append(np.array(X_i_j))\n",
    "        return X_i_b, prediction\n",
    "    \n",
    "    # def forward_per_input(self, inputs):\n",
    "    #     \"\"\"inputs-- (Tx, 10) \"\"\"   \n",
    "    #     out=[]    #(Tx, 1)\n",
    "    #     X_i_j = []  #(Tx, 10)\n",
    "    #     Tx = len(inputs)\n",
    "    #     y_prev=0\n",
    "    #     for i in range(Tx):\n",
    "    #         x = np.concatenate([inputs[i], np.array([y_prev])])\n",
    "    #         X_i_j.append(x)\n",
    "    #         x = x.T @ self.weights - self.threshold[0]\n",
    "    #         x = sigmoid(x)\n",
    "    #         out.append(x)\n",
    "\n",
    "    #     return np.array(X_i_j), out\n",
    "\n",
    "    # def backward(self, inputs, target):\n",
    "    #     \"\"\"inputs-- (B, Tx, 10)\n",
    "    #        target-- (B, Tx)\n",
    "    #         \"\"\"   \n",
    "    #     X, prediction = self.forward(inputs)\n",
    "\n",
    "    #     for i in range(len(inputs)):      # iterate over each example\n",
    "    #         delta_w = np.zeros(10)\n",
    "    #         for j in range(len(inputs[i])):     # iterate over each time\n",
    "    #             x = X[i][j]\n",
    "    #             delta_w += -self.lr * (target[i][j]-prediction[i][j]) * (x)\n",
    "    #         self.weights += delta_w\n",
    "    \n",
    "\n",
    "            \n",
    "    def backward_gpt(self, inputs, targets):\n",
    "\n",
    "        X_i_b, prediction = self.forward(inputs)\n",
    "         \n",
    "        B = len(inputs)  # Get batch size, sequence length, and feature dim\n",
    "\n",
    "        # Initialize gradients for weights and bias\n",
    "        self.weights_grad = np.zeros_like(self.weights)\n",
    "        self.threshold_grad = np.zeros_like(self.threshold)\n",
    "        sequence_lengths = [len(t) for t in targets]\n",
    "\n",
    "        # Calculate gradients for output layer (using element-wise multiplication)\n",
    "        for b in range(B):\n",
    "            Tx = sequence_lengths[b]\n",
    "            for t in range(Tx):\n",
    "                delta_t = (prediction[b][t] - targets[b][t]) * sigmoid_derivative(prediction[b][t])\n",
    "                self.weights_grad += X_i_b[b][t]*delta_t\n",
    "                self.threshold_grad += delta_t\n",
    "\n",
    "            # Backpropagate through time (using chain rule)\n",
    "            delta_prev = 0\n",
    "            for t in reversed(range(Tx)):\n",
    "                if t + 1 < Tx:\n",
    "                    delta_t = delta_prev * sigmoid_derivative(prediction[b][t]) + np.dot(delta_t, self.weights)\n",
    "                else:\n",
    "                    delta_t = delta_prev * sigmoid_derivative(prediction[b][t])\n",
    "                self.weights_grad -= X_i_b[b][t] * delta_t*0.3  # Exclude previous output\n",
    "                delta_prev = delta_t\n",
    "              \n",
    "\n",
    "            # Normalize gradients by batch size\n",
    "        self.weights_grad /= B\n",
    "        self.threshold_grad /= B\n",
    "\n",
    "            # Update weights and bias using learning rate\n",
    "        self.weights -= self.lr * self.weights_grad\n",
    "        self.threshold -= self.lr * self.threshold_grad\n",
    "\n",
    "    def calculate_loss(self, inputs, targets):\n",
    "        \n",
    "        \"\"\"\n",
    "        This function calculates the total loss for a minibatch of sequences.\n",
    "\n",
    "        Args:\n",
    "        inputs: Batch of input sequences (B, Tx_max, vec_len).\n",
    "        targets: Batch of ground truth sequences (B, Tx_max).\n",
    "\n",
    "        Returns:\n",
    "        The average loss over the minibatch.\n",
    "        \"\"\"\n",
    "        B = len(inputs)  # Get batch size, max sequence length, and feature dim\n",
    "\n",
    "        # Initialize loss to zero\n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "        _, predictions = self.forward(inputs)\n",
    "        # Forward pass for each example in the minibatch\n",
    "        for b in range(B):\n",
    "            # Calculate loss per example using cross-entropy\n",
    "          \n",
    "            loss += cross_entropy_loss(predictions[b], targets[b])\n",
    "            accuracy += np.mean(targets[b]==(predictions[b]>0.5).astype(int))\n",
    "            \n",
    "        # Average loss over the minibatch\n",
    "        return loss / B, accuracy/B\n",
    "\n",
    "\n",
    "        \n",
    "    def train(self, inputs, targets, epochs):\n",
    "\n",
    "        \"\"\"inputs-- (B, Tx, 10)\n",
    "           target-- (B, Tx)\n",
    "            \"\"\"           \n",
    "        \n",
    "        for iter in range(epochs):\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            train_loss = 0\n",
    "            val_loss = 0\n",
    "            train_accuracy = 0\n",
    "            val_accuracy = 0\n",
    "            for train_index, val_index in kf.split(inputs):\n",
    "                train_inputs, val_inputs = [inputs[i] for i in train_index], [inputs[i] for i in val_index]\n",
    "                train_targets, val_targets = [targets[i] for i in train_index], [targets[i] for i in val_index]\n",
    "                self.backward_gpt(inputs, targets)\n",
    "                delta_loss, delta_accuracy = self.calculate_loss(train_inputs,train_targets)\n",
    "                train_loss += delta_loss\n",
    "                train_accuracy += delta_accuracy\n",
    "                delta_loss, delta_accuracy = self.calculate_loss(val_inputs,val_targets )\n",
    "\n",
    "                val_loss += delta_loss\n",
    "                val_accuracy += delta_accuracy\n",
    "            print(f\"epoch: {iter}, training loss : {train_loss/5}, training accuracy: {train_accuracy/5}, validation loss: {val_loss/5}, validation accuracy: {val_accuracy/5}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SingleRecurrentPerceptron(10, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training loss : 161.4176014161432, training accuracy: 0.6300183546225444, validation loss: 161.4158874638575, validation accuracy: 0.6300179516967915\n",
      "epoch: 1, training loss : 161.05012304941351, training accuracy: 0.6300183546225444, validation loss: 161.04841692003635, validation accuracy: 0.6300179516967915\n",
      "epoch: 2, training loss : 160.68471258191852, training accuracy: 0.6300183546225444, validation loss: 160.68301572611125, validation accuracy: 0.6300179516967915\n",
      "epoch: 3, training loss : 160.32148132526467, training accuracy: 0.6370529516355663, validation loss: 160.319795153705, validation accuracy: 0.6370814353943863\n",
      "epoch: 4, training loss : 159.9605366330468, training accuracy: 0.6491132099754512, validation loss: 159.95886251272782, validation accuracy: 0.648883633835918\n",
      "epoch: 5, training loss : 159.60198176103896, training accuracy: 0.655106117831634, validation loss: 159.60032101191203, validation accuracy: 0.6553394625449076\n",
      "epoch: 6, training loss : 159.24591574604074, training accuracy: 0.6560127189966376, validation loss: 159.2442696379606, validation accuracy: 0.6560123466005534\n",
      "epoch: 7, training loss : 158.8924333030564, training accuracy: 0.6576068948274238, validation loss: 158.8908030530879, validation accuracy: 0.6576180582518567\n",
      "epoch: 8, training loss : 158.5416247405912, training accuracy: 0.6590758932421521, validation loss: 158.5400115106248, validation accuracy: 0.6590755005462274\n",
      "epoch: 9, training loss : 158.1935758935281, training accuracy: 0.6590758932421521, validation loss: 158.191980788242, validation accuracy: 0.6590755005462274\n",
      "epoch: 10, training loss : 157.84836807312578, training accuracy: 0.6590758932421521, validation loss: 157.8467921382736, validation accuracy: 0.6590755005462274\n",
      "epoch: 11, training loss : 157.50607803351423, training accuracy: 0.6590758932421521, validation loss: 157.50452225453117, validation accuracy: 0.6590755005462274\n",
      "epoch: 12, training loss : 157.1667779540107, training accuracy: 0.6590758932421521, validation loss: 157.16524325493378, validation accuracy: 0.6590755005462274\n",
      "epoch: 13, training loss : 156.8305354365373, training accuracy: 0.6590758932421521, validation loss: 156.82902267922836, validation accuracy: 0.6590755005462274\n",
      "epoch: 14, training loss : 156.4974135173581, training accuracy: 0.6590758932421521, validation loss: 156.4959235010128, validation accuracy: 0.6590755005462274\n",
      "epoch: 15, training loss : 156.16747069230394, training accuracy: 0.6590758932421521, validation loss: 156.16600415325192, validation accuracy: 0.6590755005462274\n",
      "epoch: 16, training loss : 155.84076095468382, training accuracy: 0.6590758932421521, validation loss: 155.83931856643215, validation accuracy: 0.6590755005462274\n",
      "epoch: 17, training loss : 155.51733384495265, training accuracy: 0.6590758932421521, validation loss: 155.51591621849548, validation accuracy: 0.6590755005462274\n",
      "epoch: 18, training loss : 155.19723451131483, training accuracy: 0.6590758932421521, validation loss: 155.1958421956659, validation accuracy: 0.6590755005462274\n",
      "epoch: 19, training loss : 154.88050378034208, training accuracy: 0.6590758932421521, validation loss: 154.87913726329103, validation accuracy: 0.6590755005462274\n",
      "epoch: 20, training loss : 154.56717823675234, training accuracy: 0.6590758932421521, validation loss: 154.56583794581525, validation accuracy: 0.6590755005462274\n",
      "epoch: 21, training loss : 154.25729031144527, training accuracy: 0.6590758932421521, validation loss: 154.25597661500808, validation accuracy: 0.6590755005462274\n",
      "epoch: 22, training loss : 153.95086837697676, training accuracy: 0.6590758932421521, validation loss: 153.9495815855983, validation accuracy: 0.6590755005462274\n",
      "epoch: 23, training loss : 153.64793684959545, training accuracy: 0.6590758932421521, validation loss: 153.646677217469, validation accuracy: 0.6590755005462274\n",
      "epoch: 24, training loss : 153.34851629705173, training accuracy: 0.6590758932421521, validation loss: 153.3472840236007, validation accuracy: 0.6590755005462274\n",
      "epoch: 25, training loss : 153.05262355139325, training accuracy: 0.6590758932421521, validation loss: 153.05141878298258, validation accuracy: 0.6590755005462274\n",
      "epoch: 26, training loss : 152.760271825969, training accuracy: 0.6590758932421521, validation loss: 152.75909465773157, validation accuracy: 0.6590755005462274\n",
      "epoch: 27, training loss : 152.4714708359641, training accuracy: 0.6590758932421521, validation loss: 152.47032131370457, validation accuracy: 0.6590755005462274\n",
      "epoch: 28, training loss : 152.1862269217294, training accuracy: 0.6590758932421521, validation loss: 152.18510504391764, validation accuracy: 0.6590755005462274\n",
      "epoch: 29, training loss : 151.90454317431997, training accuracy: 0.6590758932421521, validation loss: 151.90344889413208, validation accuracy: 0.6590755005462274\n",
      "epoch: 30, training loss : 151.62641956257744, training accuracy: 0.6590758932421521, validation loss: 151.62535279000073, validation accuracy: 0.6590755005462274\n",
      "epoch: 31, training loss : 151.35185306124302, training accuracy: 0.6590758932421521, validation loss: 151.35081366520575, validation accuracy: 0.6590755005462274\n"
     ]
    }
   ],
   "source": [
    "model.train(X_train_ho, y_train, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02747697,  0.27829624,  0.80810796, -0.3781824 ,  1.6563038 ,\n",
       "       -0.59600257,  0.37237197,  1.10109364,  1.15016423, -1.11687127])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_mapping = {1: \"NN\", 2: \"DT\", 3: \"JJ\", 4: \"OT\"}\n",
    "\n",
    "# Calculate the number of unique POS tags\n",
    "num_unique_tags = len(tag_mapping)\n",
    "\n",
    "# Preprocess training data\n",
    "# Preprocess training data\n",
    "train_sentences = []\n",
    "train_labels = []\n",
    "for entry in train_data:\n",
    "    tokens = entry['tokens']\n",
    "    pos_tags = entry['pos_tags']\n",
    "    chunk_tags = entry['chunk_tags']\n",
    "    \n",
    "    # Convert POS tags to one-hot encoded representation\n",
    "    pos_tags_one_hot = np.zeros((len(pos_tags), num_unique_tags))\n",
    "    for i, tag in enumerate(pos_tags):\n",
    "        pos_tags_one_hot[i, tag - 1] = 1  # Subtract 1 to account for 0-based indexing\n",
    "    \n",
    "    # Flatten one-hot encoded representation\n",
    "    flattened_tags = pos_tags_one_hot.flatten()\n",
    "    \n",
    "    train_sentences.append(flattened_tags)\n",
    "    train_labels.append(chunk_tags)\n",
    "\n",
    "\n",
    "# Initialize and train the single recurrent perceptron\n",
    "# Initialize and train the single recurrent perceptron\n",
    "input_size = len(train_sentences[0])  # Get input size from the first sample\n",
    "output_size = 2  # Binary classification (1 for chunk, 0 for not chunk)\n",
    "perceptron = SingleRecurrentPerceptron(input_size, output_size)\n",
    "perceptron.train(train_sentences, train_labels, epochs=10)\n",
    "\n",
    "\n",
    "\n",
    "# Load test data\n",
    "with open('test.jsonl', 'r') as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Preprocess test data\n",
    "test_sentences = []\n",
    "test_labels = []\n",
    "for entry in test_data:\n",
    "    tokens = entry['tokens']\n",
    "    pos_tags = entry['pos_tags']\n",
    "    chunk_tags = entry['chunk_tags']\n",
    "    \n",
    "    # Convert POS tags to one-hot encoded representation\n",
    "    pos_tags_one_hot = np.zeros((len(pos_tags), len(tag_mapping)))\n",
    "    for i, tag in enumerate(pos_tags):\n",
    "        pos_tags_one_hot[i, tag - 1] = 1  # Subtract 1 to account for 0-based indexing\n",
    "    \n",
    "    test_sentences.append(pos_tags_one_hot)\n",
    "    test_labels.append(chunk_tags)\n",
    "\n",
    "# Evaluate the trained perceptron\n",
    "predictions = perceptron.predict(test_sentences)\n",
    "\n",
    "# Assuming we have some evaluation function to compute accuracy\n",
    "# Let's assume a simple accuracy calculation for demonstration\n",
    "def accuracy(predictions, targets):\n",
    "    correct = 0\n",
    "    total = len(predictions)\n",
    "    for pred, target in zip(predictions, targets):\n",
    "        pred_labels = [1 if p > 0 else 0 for p in pred]\n",
    "        if pred_labels == target:\n",
    "            correct += 1\n",
    "    return correct / total\n",
    "\n",
    "acc = accuracy(predictions, test_labels)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice_linux",
   "language": "python",
   "name": "practice_linux"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
