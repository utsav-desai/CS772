{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from activation import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(\"data\", \"train.jsonl\")\n",
    "test_path = os.path.join(\"data\",\"test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(train_path, test_path):\n",
    "    with open(train_path, 'r') as f:\n",
    "        train_data = [json.loads(line) for line in f]\n",
    "    # Preprocess training data\n",
    "    train_sentences = []\n",
    "    train_labels = []\n",
    "    for entry in train_data:\n",
    "        tokens = entry['tokens']\n",
    "        pos_tags = entry['pos_tags']\n",
    "        chunk_tags = np.array(entry['chunk_tags'])\n",
    "        \n",
    "        train_sentences.append(pos_tags)\n",
    "        train_labels.append(chunk_tags)\n",
    "\n",
    "    with open(test_path, 'r') as f:\n",
    "        test_data = [json.loads(line) for line in f]\n",
    "    # Preprocess test data\n",
    "    test_sentences = []\n",
    "    test_labels = []\n",
    "    for entry in test_data:\n",
    "        tokens = entry['tokens']\n",
    "        pos_tags = entry['pos_tags']\n",
    "        chunk_tags = np.array(entry['chunk_tags'])\n",
    "        \n",
    "        test_sentences.append(pos_tags)\n",
    "        test_labels.append(chunk_tags)\n",
    "    \n",
    "    return train_sentences, test_sentences,  train_labels, test_labels\n",
    "\n",
    "X_train, X_test, y_train, y_test= fetch_data(train_path, test_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to process the POS data\n",
    "into One hot vectors,\n",
    "that is :\n",
    "Input: [DT/TT/NN/OT]    Shape: (1)\n",
    "Output : [DT TT NN OT]  Shape: (4,1)\n",
    "\"\"\"\n",
    "def one_hot_encode(input_list):\n",
    "    encoded_list = []\n",
    "    for item in input_list:\n",
    "        one_hot_vector = np.zeros(4)\n",
    "        one_hot_vector[item - 1] = 1  # Adjust index to start from 0\n",
    "        encoded_list.append(one_hot_vector.tolist())\n",
    "    return np.array(encoded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to process the POS data\n",
    "into Recurrent Perceptron input format,\n",
    "that is :\n",
    "Input : [DT TT NN OT]\n",
    "\"\"\"\n",
    "def into_ho(X_train):\n",
    "    X_train_ho = []\n",
    "    for i in range(len(X_train)):\n",
    "        X = one_hot_encode(X_train[i])\n",
    "        temp = []\n",
    "        for j in range(len(X)):\n",
    "            if j==0:\n",
    "                temp.append(np.concatenate([np.array([1.0,0.0,0.0,0.0,0.0]), X[j]]))\n",
    "            else:\n",
    "                temp.append((np.concatenate([np.array([0]), X[j-1], X[j]] )))\n",
    "        X_train_ho.append(np.array(temp))\n",
    "    return X_train_ho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 3, 1, 4, 4, 3, 1, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "one_hot_encode(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ho = into_ho(X_train)\n",
    "X_test_ho = into_ho(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Perceptron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some representation which will help understand this code base.\n",
    "B       - Batch Size\n",
    "Tx      - Length of Input string\n",
    "X_i_b   -   \n",
    "X_i_j   -   \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class SingleRecurrentPerceptron:\n",
    "    def __init__(self, vec_len=10, lr=0.05):\n",
    "          \n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.random.randn(vec_len)\n",
    "        self.threshold = np.random.randn(1)\n",
    "        self.lr = lr\n",
    "\n",
    "   \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"inputs-- (B, Tx, 10)\"\"\"   \n",
    "        prediction= []    #(B, Tx)\n",
    "        X_i_b = []      #(B, Tx, 10)\n",
    "        for j in range(len(inputs)):\n",
    "            out=[]\n",
    "            X_i_j = []\n",
    "            Tx, _ = inputs[j].shape\n",
    "            y_prev=0\n",
    "            for i in range(Tx):\n",
    "                x = np.concatenate([inputs[j][i], np.array([y_prev])])\n",
    "                X_i_j.append(x)\n",
    "                net = x.T @ self.weights - self.threshold[0]\n",
    "                oi = sigmoid(net)\n",
    "                y_prev = oi\n",
    "                out.append(oi)\n",
    "            prediction.append(np.array(out))\n",
    "            X_i_b.append(np.array(X_i_j))\n",
    "        return X_i_b, prediction\n",
    "\n",
    "    # def backward(self, inputs, target):\n",
    "    #     \"\"\"inputs-- (B, Tx, 10)\n",
    "    #        target-- (B, Tx)\n",
    "    #         \"\"\"   \n",
    "    #     X, prediction = self.forward(inputs)\n",
    "\n",
    "    #     for i in range(len(inputs)):      # iterate over each example\n",
    "    #         delta_w = np.zeros(10)\n",
    "    #         for j in range(len(inputs[i])):     # iterate over each time\n",
    "    #             x = X[i][j]\n",
    "    #             delta_w += -self.lr * (target[i][j]-prediction[i][j]) * (x)\n",
    "    #         self.weights += delta_w\n",
    "    \n",
    "\n",
    "            \n",
    "    def backward_gpt(self, inputs, targets):\n",
    "\n",
    "        X_i_b, prediction = self.forward(inputs)\n",
    "         \n",
    "        B = len(inputs)  # Get batch size, sequence length, and feature dim\n",
    "\n",
    "        # Initialize gradients for weights and bias\n",
    "        self.weights_grad = np.zeros_like(self.weights)\n",
    "        self.threshold_grad = np.zeros_like(self.threshold)\n",
    "        sequence_lengths = [len(t) for t in targets]\n",
    "\n",
    "        # Calculate gradients for output layer (using element-wise multiplication)\n",
    "        for b in range(B):\n",
    "            Tx = sequence_lengths[b]\n",
    "            for t in range(Tx):\n",
    "                delta_t = (prediction[b][t] - targets[b][t]) * sigmoid_derivative(prediction[b][t])\n",
    "                self.weights_grad += X_i_b[b][t]*delta_t\n",
    "                self.threshold_grad += delta_t\n",
    "\n",
    "            # BPTT (using chain rule)\n",
    "            delta_prev = 0\n",
    "            for t in reversed(range(Tx)):\n",
    "                if t + 1 < Tx:\n",
    "                    delta_t = delta_prev * sigmoid_derivative(prediction[b][t]) + np.dot(delta_t, self.weights)\n",
    "                else:\n",
    "                    delta_t = delta_prev * sigmoid_derivative(prediction[b][t])\n",
    "                self.weights_grad -= X_i_b[b][t] * delta_t*0.3  # Exclude previous output\n",
    "                delta_prev = delta_t\n",
    "\n",
    "        # Normalize gradients by batch size\n",
    "        self.weights_grad /= B\n",
    "        self.threshold_grad /= B\n",
    "\n",
    "        # Update weights and bias\n",
    "        self.weights -= self.lr * self.weights_grad\n",
    "        self.threshold -= self.lr * self.threshold_grad\n",
    "\n",
    "    def calculate_loss(self, inputs, targets):\n",
    "        \n",
    "        \"\"\"\n",
    "        This function calculates the total loss for a minibatch of sequences.\n",
    "\n",
    "        Args:\n",
    "        inputs: Batch of input sequences (B, Tx_max, vec_len).\n",
    "        targets: Batch of ground truth sequences (B, Tx_max).\n",
    "\n",
    "        Returns:\n",
    "        The average loss over the minibatch.\n",
    "        \"\"\"\n",
    "        B = len(inputs)  # Get batch size, max sequence length, and feature dim\n",
    "\n",
    "        # Initialize loss to zero\n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "        _, predictions = self.forward(inputs)\n",
    "        # Forward pass for each example in the minibatch\n",
    "        for b in range(B):\n",
    "            # Calculate loss per example using cross-entropy\n",
    "          \n",
    "            loss += cross_entropy_loss(predictions[b], targets[b])\n",
    "            accuracy += np.mean(targets[b]==(predictions[b]>0.5).astype(int))\n",
    "            \n",
    "        # Average loss over the minibatch\n",
    "        return loss / B, accuracy/B\n",
    "\n",
    "\n",
    "        \n",
    "    def train(self, inputs, targets, epochs):\n",
    "\n",
    "        \"\"\"inputs-- (B, Tx, 10)\n",
    "           target-- (B, Tx)\n",
    "            \"\"\"           \n",
    "        \n",
    "        for iter in range(epochs):\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            train_loss = 0\n",
    "            val_loss = 0\n",
    "            train_accuracy = 0\n",
    "            val_accuracy = 0\n",
    "            for train_index, val_index in kf.split(inputs):\n",
    "                train_inputs, val_inputs = [inputs[i] for i in train_index], [inputs[i] for i in val_index]\n",
    "                train_targets, val_targets = [targets[i] for i in train_index], [targets[i] for i in val_index]\n",
    "                self.backward_gpt(inputs, targets)\n",
    "                delta_loss, delta_accuracy = self.calculate_loss(train_inputs,train_targets)\n",
    "                train_loss += delta_loss\n",
    "                train_accuracy += delta_accuracy\n",
    "                delta_loss, delta_accuracy = self.calculate_loss(val_inputs,val_targets )\n",
    "\n",
    "                val_loss += delta_loss\n",
    "                val_accuracy += delta_accuracy\n",
    "            print(f\"epoch: {iter:.2f}, training loss : {train_loss/5:.2f}, training accuracy: {train_accuracy*100/5:.2f}%, validation loss: {val_loss/5:.2f}, validation accuracy: {val_accuracy*100/5:.2f}%\")\n",
    "    \n",
    "\n",
    "    def save(self,path=\"model.pkl\"):\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    def load(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            self = pickle.load(f) \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SingleRecurrentPerceptron(vec_len=10, lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(X_train_ho, y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(path=\"models/recurrent_perceptron.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.84747142,  0.02764599, -0.05904373,  0.60626922,  0.74619764,\n",
       "        0.45700575,  0.38727425,  1.97348169,  0.6750649 ,  1.00674874])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SingleRecurrentPerceptron()\n",
    "model = model.load(\"models/recurrent_perceptron.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m train_sentences \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_data\u001b[49m:\n\u001b[1;32m     11\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m     pos_tags \u001b[38;5;241m=\u001b[39m entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos_tags\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "tag_mapping = {1: \"NN\", 2: \"DT\", 3: \"JJ\", 4: \"OT\"}\n",
    "\n",
    "# Calculate the number of unique POS tags\n",
    "num_unique_tags = len(tag_mapping)\n",
    "\n",
    "# Preprocess training data\n",
    "# Preprocess training data\n",
    "train_sentences = []\n",
    "train_labels = []\n",
    "for entry in train_data:\n",
    "    tokens = entry['tokens']\n",
    "    pos_tags = entry['pos_tags']\n",
    "    chunk_tags = entry['chunk_tags']\n",
    "    \n",
    "    # Convert POS tags to one-hot encoded representation\n",
    "    pos_tags_one_hot = np.zeros((len(pos_tags), num_unique_tags))\n",
    "    for i, tag in enumerate(pos_tags):\n",
    "        pos_tags_one_hot[i, tag - 1] = 1  # Subtract 1 to account for 0-based indexing\n",
    "    \n",
    "    # Flatten one-hot encoded representation\n",
    "    flattened_tags = pos_tags_one_hot.flatten()\n",
    "    \n",
    "    train_sentences.append(flattened_tags)\n",
    "    train_labels.append(chunk_tags)\n",
    "\n",
    "\n",
    "# Initialize and train the single recurrent perceptron\n",
    "# Initialize and train the single recurrent perceptron\n",
    "input_size = len(train_sentences[0])  # Get input size from the first sample\n",
    "output_size = 2  # Binary classification (1 for chunk, 0 for not chunk)\n",
    "perceptron = SingleRecurrentPerceptron(input_size, output_size)\n",
    "perceptron.train(train_sentences, train_labels, epochs=10)\n",
    "\n",
    "\n",
    "\n",
    "# Load test data\n",
    "with open('test.jsonl', 'r') as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Preprocess test data\n",
    "test_sentences = []\n",
    "test_labels = []\n",
    "for entry in test_data:\n",
    "    tokens = entry['tokens']\n",
    "    pos_tags = entry['pos_tags']\n",
    "    chunk_tags = entry['chunk_tags']\n",
    "    \n",
    "    # Convert POS tags to one-hot encoded representation\n",
    "    pos_tags_one_hot = np.zeros((len(pos_tags), len(tag_mapping)))\n",
    "    for i, tag in enumerate(pos_tags):\n",
    "        pos_tags_one_hot[i, tag - 1] = 1  # Subtract 1 to account for 0-based indexing\n",
    "    \n",
    "    test_sentences.append(pos_tags_one_hot)\n",
    "    test_labels.append(chunk_tags)\n",
    "\n",
    "# Evaluate the trained perceptron\n",
    "predictions = perceptron.predict(test_sentences)\n",
    "\n",
    "# Assuming we have some evaluation function to compute accuracy\n",
    "# Let's assume a simple accuracy calculation for demonstration\n",
    "def accuracy(predictions, targets):\n",
    "    correct = 0\n",
    "    total = len(predictions)\n",
    "    for pred, target in zip(predictions, targets):\n",
    "        pred_labels = [1 if p > 0 else 0 for p in pred]\n",
    "        if pred_labels == target:\n",
    "            correct += 1\n",
    "    return correct / total\n",
    "\n",
    "acc = accuracy(predictions, test_labels)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "Thanks for being a Gradio user! If you have questions or feedback, please join our Discord server and chat with us: https://discord.gg/feTf9x3ZSB\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/29 18:10:42 [W] [service.go:132] login to server failed: dial tcp 44.237.78.176:7000: i/o timeout\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 3, 1, 4, 4, 3, 1, 4]\n"
     ]
    }
   ],
   "source": [
    "def forward_per_input(inputs):\n",
    "    # inputs = one_hot_encode(inputs)\n",
    "    inputs = into_ho([inputs])[0]\n",
    "    \"\"\"inputs-- (Tx, 10) \"\"\"   \n",
    "    out=[]    #(Tx, 1)\n",
    "    X_i_j = []  #(Tx, 10)\n",
    "    Tx = len(inputs)\n",
    "    y_prev=0\n",
    "    for i in range(Tx):\n",
    "        x = np.concatenate([inputs[i], np.array([y_prev])])\n",
    "        X_i_j.append(x)\n",
    "        x = x.T @ model.weights - model.threshold[0]\n",
    "        x = sigmoid(x)\n",
    "        out.append(x)\n",
    "    out = (np.array(out) > 0.5) * 1\n",
    "    return out\n",
    "\n",
    "def predict(input_string):\n",
    "    inp = input_string.split(',')\n",
    "    input_string = [int(i) for i in inp]\n",
    "    print(input_string)\n",
    "    result = forward_per_input(input_string)\n",
    "    out = [str(i) for i in result]\n",
    "    out = ' '.join(out)\n",
    "    return out\n",
    "\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "@app.route(\"/classify\", methods=[\"POST\"])\n",
    "def classify():\n",
    "    data = request.get_json()\n",
    "    input_string = data[\"inputString\"]\n",
    "    out = predict(input_string)\n",
    "    return jsonify({\"result\": out})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface = gr.Interface(fn=predict, inputs=\"text\", outputs=\"text\")\n",
    "    iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sem8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
