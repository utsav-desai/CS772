{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def fetch_data():\n",
    "    with open(path, 'r') as f:\n",
    "        train_data = [json.loads(line) for line in f]\n",
    "\n",
    "    tag_mapping = {1: \"NN\", 2: \"DT\", 3: \"JJ\", 4: \"OT\"}\n",
    "\n",
    "    # Preprocess training data\n",
    "    train_sentences = []\n",
    "    train_labels = []\n",
    "    for entry in train_data:\n",
    "        tokens = entry['tokens']\n",
    "        pos_tags = entry['pos_tags']\n",
    "        chunk_tags = entry['chunk_tags']\n",
    "        \n",
    "        train_sentences.append(pos_tags)\n",
    "        train_labels.append(chunk_tags)\n",
    "    \n",
    "    return train_sentences, train_labels\n",
    "\n",
    "train_data = fetch_data(\"./train.jsonl\")\n",
    "test_data = import_data(\"./test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: [[1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n",
      "Prediction: [[0.]\n",
      " [0.]]\n",
      "Inputs: [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "Target shape: (9, 1)\n",
      "Prediction shape: (2, 1)\n",
      "Inputs shape: (36, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (9,1) (2,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m output_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Binary classification (1 for chunk, 0 for not chunk)\u001b[39;00m\n\u001b[1;32m     82\u001b[0m perceptron \u001b[38;5;241m=\u001b[39m SingleRecurrentPerceptron(input_size, output_size)\n\u001b[0;32m---> 83\u001b[0m \u001b[43mperceptron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Load test data\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[23], line 37\u001b[0m, in \u001b[0;36mSingleRecurrentPerceptron.train\u001b[0;34m(self, inputs, targets, epochs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs)):\n\u001b[0;32m---> 37\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 29\u001b[0m, in \u001b[0;36mSingleRecurrentPerceptron.backward\u001b[0;34m(self, inputs, target)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, prediction\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, inputs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 29\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m error \u001b[38;5;241m*\u001b[39m inputs\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m error\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (9,1) (2,1) "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "class SingleRecurrentPerceptron:\n",
    "    def __init__(self, input_size, output_size, learning_rate=0.1):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.zeros((output_size, input_size))\n",
    "        self.bias = np.zeros(output_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        return np.dot(self.weights, inputs)[:, np.newaxis] + self.bias[:, np.newaxis]\n",
    "    \n",
    "    def backward(self, inputs, target):\n",
    "        prediction = self.forward(inputs)\n",
    "        prediction = np.array(prediction)  # Ensure prediction is a NumPy array\n",
    "        target = np.array(target)  # Ensure target is a NumPy array\n",
    "        target = target[:, np.newaxis]  # Reshape target to match prediction's shape\n",
    "        inputs = inputs[:, np.newaxis]  # Reshape inputs to match prediction's shape\n",
    "        print(\"Target:\", target)\n",
    "        print(\"Prediction:\", prediction)\n",
    "        print(\"Inputs:\", inputs)\n",
    "        print(\"Target shape:\", target.shape)\n",
    "        print(\"Prediction shape:\", prediction.shape)\n",
    "        print(\"Inputs shape:\", inputs.shape)\n",
    "        error = target - prediction\n",
    "        self.weights += self.learning_rate * error * inputs\n",
    "        self.bias += self.learning_rate * error\n",
    "\n",
    "        \n",
    "    def train(self, inputs, targets, epochs):\n",
    "        for _ in range(epochs):\n",
    "            for i in range(len(inputs)):\n",
    "                self.backward(inputs[i], targets[i])\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        predictions = []\n",
    "        for input_data in inputs:\n",
    "            prediction = self.forward(input_data)\n",
    "            predictions.append(prediction)\n",
    "        return predictions\n",
    "\n",
    "# Load training data\n",
    "with open('train.jsonl', 'r') as f:\n",
    "    train_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Map numerical values to POS tags\n",
    "# Map numerical values to POS tags\n",
    "tag_mapping = {1: \"NN\", 2: \"DT\", 3: \"JJ\", 4: \"OT\"}\n",
    "\n",
    "# Calculate the number of unique POS tags\n",
    "num_unique_tags = len(tag_mapping)\n",
    "\n",
    "# Preprocess training data\n",
    "# Preprocess training data\n",
    "train_sentences = []\n",
    "train_labels = []\n",
    "for entry in train_data:\n",
    "    tokens = entry['tokens']\n",
    "    pos_tags = entry['pos_tags']\n",
    "    chunk_tags = entry['chunk_tags']\n",
    "    \n",
    "    # Convert POS tags to one-hot encoded representation\n",
    "    pos_tags_one_hot = np.zeros((len(pos_tags), num_unique_tags))\n",
    "    for i, tag in enumerate(pos_tags):\n",
    "        pos_tags_one_hot[i, tag - 1] = 1  # Subtract 1 to account for 0-based indexing\n",
    "    \n",
    "    # Flatten one-hot encoded representation\n",
    "    flattened_tags = pos_tags_one_hot.flatten()\n",
    "    \n",
    "    train_sentences.append(flattened_tags)\n",
    "    train_labels.append(chunk_tags)\n",
    "\n",
    "\n",
    "# Initialize and train the single recurrent perceptron\n",
    "# Initialize and train the single recurrent perceptron\n",
    "input_size = len(train_sentences[0])  # Get input size from the first sample\n",
    "output_size = 2  # Binary classification (1 for chunk, 0 for not chunk)\n",
    "perceptron = SingleRecurrentPerceptron(input_size, output_size)\n",
    "perceptron.train(train_sentences, train_labels, epochs=10)\n",
    "\n",
    "\n",
    "\n",
    "# Load test data\n",
    "with open('test.jsonl', 'r') as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Preprocess test data\n",
    "test_sentences = []\n",
    "test_labels = []\n",
    "for entry in test_data:\n",
    "    tokens = entry['tokens']\n",
    "    pos_tags = entry['pos_tags']\n",
    "    chunk_tags = entry['chunk_tags']\n",
    "    \n",
    "    # Convert POS tags to one-hot encoded representation\n",
    "    pos_tags_one_hot = np.zeros((len(pos_tags), len(tag_mapping)))\n",
    "    for i, tag in enumerate(pos_tags):\n",
    "        pos_tags_one_hot[i, tag - 1] = 1  # Subtract 1 to account for 0-based indexing\n",
    "    \n",
    "    test_sentences.append(pos_tags_one_hot)\n",
    "    test_labels.append(chunk_tags)\n",
    "\n",
    "# Evaluate the trained perceptron\n",
    "predictions = perceptron.predict(test_sentences)\n",
    "\n",
    "# Assuming we have some evaluation function to compute accuracy\n",
    "# Let's assume a simple accuracy calculation for demonstration\n",
    "def accuracy(predictions, targets):\n",
    "    correct = 0\n",
    "    total = len(predictions)\n",
    "    for pred, target in zip(predictions, targets):\n",
    "        pred_labels = [1 if p > 0 else 0 for p in pred]\n",
    "        if pred_labels == target:\n",
    "            correct += 1\n",
    "    return correct / total\n",
    "\n",
    "acc = accuracy(predictions, test_labels)\n",
    "print(\"Accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
