{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_348974/2556754689.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# from faker import Faker\n",
    "import random\n",
    "np.random.seed()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "palindromes = []\n",
    "for i in range(1024):\n",
    "    binary_string = format(i, '010b')\n",
    "    x.append(binary_string)\n",
    "    if binary_string == binary_string[::-1]:\n",
    "        palindromes.append(binary_string)\n",
    "x = np.array(x)\n",
    "palindromes = np.array(palindromes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['0000000000', '0000110000', '0001001000', '0001111000',\n",
       "       '0010000100', '0010110100', '0011001100', '0011111100',\n",
       "       '0100000010', '0100110010', '0101001010', '0101111010',\n",
       "       '0110000110', '0110110110', '0111001110', '0111111110',\n",
       "       '1000000001', '1000110001', '1001001001', '1001111001',\n",
       "       '1010000101', '1010110101', '1011001101', '1011111101',\n",
       "       '1100000011', '1100110011', '1101001011', '1101111011',\n",
       "       '1110000111', '1110110111', '1111001111', '1111111111'],\n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(palindromes))\n",
    "palindromes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['1011001101', '1010000101', '1100110011', '0001111000',\n",
       "       '0111111110', '0101111010', '0001001000', '1111001111',\n",
       "       '0101001010', '0011001100', '1110000111', '0110000110',\n",
       "       '1000000001', '1000110001', '0000110000', '1101111011',\n",
       "       '0010000100', '0000000000', '1001111001', '0011111100',\n",
       "       '0100110010', '1011111101', '1110110111', '0010110100',\n",
       "       '1001001001', '1100000011', '1111111111', '0110110110',\n",
       "       '0100000010', '0111001110', '1010110101', '1101001011'],\n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(palindromes)\n",
    "print(len(palindromes))\n",
    "palindromes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1184\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    x = np.concatenate((x, palindromes))\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for binary_string in x:\n",
    "    y.append(binary_string == binary_string[::-1])\n",
    "y = np.array(y)\n",
    "permutation_index = np.random.permutation(len(x))\n",
    "x = x[permutation_index]\n",
    "y = y[permutation_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-1 * x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.array([1, 2, -3])\n",
    "# b = np.array([2, 3, 4])\n",
    "# a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 10000\n",
    "\n",
    "input_size = 10\n",
    "hidden_layer_size = 4\n",
    "output_size = 1\n",
    "\n",
    "weights_ih = np.random.rand(input_size, hidden_layer_size)\n",
    "weights_ho = np.random.rand(hidden_layer_size, output_size)\n",
    "bias_ih = np.random.rand(1, hidden_layer_size)\n",
    "bias_ho = np.random.rand(1, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : loss [0.82641608]\n",
      "Epoch 1 : loss [0.82609208]\n",
      "Epoch 2 : loss [0.82576872]\n",
      "Epoch 3 : loss [0.825446]\n",
      "Epoch 4 : loss [0.82512391]\n",
      "Epoch 5 : loss [0.82480246]\n",
      "Epoch 6 : loss [0.82448164]\n",
      "Epoch 7 : loss [0.82416145]\n",
      "Epoch 8 : loss [0.82384189]\n",
      "Epoch 9 : loss [0.82352295]\n",
      "Epoch 10 : loss [0.82320464]\n",
      "Epoch 11 : loss [0.82288695]\n",
      "Epoch 12 : loss [0.82256987]\n",
      "Epoch 13 : loss [0.82225341]\n",
      "Epoch 14 : loss [0.82193756]\n",
      "Epoch 15 : loss [0.82162233]\n",
      "Epoch 16 : loss [0.8213077]\n",
      "Epoch 17 : loss [0.82099368]\n",
      "Epoch 18 : loss [0.82068026]\n",
      "Epoch 19 : loss [0.82036745]\n",
      "Epoch 20 : loss [0.82005523]\n",
      "Epoch 21 : loss [0.81974362]\n",
      "Epoch 22 : loss [0.81943259]\n",
      "Epoch 23 : loss [0.81912216]\n",
      "Epoch 24 : loss [0.81881233]\n",
      "Epoch 25 : loss [0.81850308]\n",
      "Epoch 26 : loss [0.81819441]\n",
      "Epoch 27 : loss [0.81788633]\n",
      "Epoch 28 : loss [0.81757884]\n",
      "Epoch 29 : loss [0.81727192]\n",
      "Epoch 30 : loss [0.81696558]\n",
      "Epoch 31 : loss [0.81665982]\n",
      "Epoch 32 : loss [0.81635463]\n",
      "Epoch 33 : loss [0.81605002]\n",
      "Epoch 34 : loss [0.81574597]\n",
      "Epoch 35 : loss [0.81544249]\n",
      "Epoch 36 : loss [0.81513958]\n",
      "Epoch 37 : loss [0.81483723]\n",
      "Epoch 38 : loss [0.81453544]\n",
      "Epoch 39 : loss [0.81423421]\n",
      "Epoch 40 : loss [0.81393354]\n",
      "Epoch 41 : loss [0.81363342]\n",
      "Epoch 42 : loss [0.81333386]\n",
      "Epoch 43 : loss [0.81303485]\n",
      "Epoch 44 : loss [0.81273639]\n",
      "Epoch 45 : loss [0.81243848]\n",
      "Epoch 46 : loss [0.81214111]\n",
      "Epoch 47 : loss [0.81184429]\n",
      "Epoch 48 : loss [0.81154801]\n",
      "Epoch 49 : loss [0.81125227]\n",
      "Epoch 50 : loss [0.81095706]\n",
      "Epoch 51 : loss [0.8106624]\n",
      "Epoch 52 : loss [0.81036827]\n",
      "Epoch 53 : loss [0.81007467]\n",
      "Epoch 54 : loss [0.8097816]\n",
      "Epoch 55 : loss [0.80948906]\n",
      "Epoch 56 : loss [0.80919705]\n",
      "Epoch 57 : loss [0.80890556]\n",
      "Epoch 58 : loss [0.8086146]\n",
      "Epoch 59 : loss [0.80832415]\n",
      "Epoch 60 : loss [0.80803423]\n",
      "Epoch 61 : loss [0.80774483]\n",
      "Epoch 62 : loss [0.80745594]\n",
      "Epoch 63 : loss [0.80716757]\n",
      "Epoch 64 : loss [0.80687971]\n",
      "Epoch 65 : loss [0.80659236]\n",
      "Epoch 66 : loss [0.80630552]\n",
      "Epoch 67 : loss [0.80601919]\n",
      "Epoch 68 : loss [0.80573336]\n",
      "Epoch 69 : loss [0.80544804]\n",
      "Epoch 70 : loss [0.80516322]\n",
      "Epoch 71 : loss [0.8048789]\n",
      "Epoch 72 : loss [0.80459508]\n",
      "Epoch 73 : loss [0.80431175]\n",
      "Epoch 74 : loss [0.80402893]\n",
      "Epoch 75 : loss [0.8037466]\n",
      "Epoch 76 : loss [0.80346476]\n",
      "Epoch 77 : loss [0.80318341]\n",
      "Epoch 78 : loss [0.80290255]\n",
      "Epoch 79 : loss [0.80262217]\n",
      "Epoch 80 : loss [0.80234229]\n",
      "Epoch 81 : loss [0.80206288]\n",
      "Epoch 82 : loss [0.80178397]\n",
      "Epoch 83 : loss [0.80150553]\n",
      "Epoch 84 : loss [0.80122757]\n",
      "Epoch 85 : loss [0.80095009]\n",
      "Epoch 86 : loss [0.80067308]\n",
      "Epoch 87 : loss [0.80039655]\n",
      "Epoch 88 : loss [0.8001205]\n",
      "Epoch 89 : loss [0.79984491]\n",
      "Epoch 90 : loss [0.7995698]\n",
      "Epoch 91 : loss [0.79929515]\n",
      "Epoch 92 : loss [0.79902098]\n",
      "Epoch 93 : loss [0.79874726]\n",
      "Epoch 94 : loss [0.79847402]\n",
      "Epoch 95 : loss [0.79820123]\n",
      "Epoch 96 : loss [0.79792891]\n",
      "Epoch 97 : loss [0.79765704]\n",
      "Epoch 98 : loss [0.79738564]\n",
      "Epoch 99 : loss [0.79711469]\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.5\n",
    "NUM_EPOCHS = 100\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    tot_loss = 0\n",
    "    \n",
    "    for i, s in enumerate(x):\n",
    "        inp = np.reshape(np.array([int(char) for char in s]), (1, input_size))\n",
    "        # print(inp.shape)\n",
    "\n",
    "        hlayer_logits = np.dot(inp, weights_ih) + bias_ih\n",
    "        # print(\"hlayer_logits\",hlayer_logits.shape)\n",
    "        hlayer_output = sigmoid(hlayer_logits)\n",
    "        # print(\"hlayer_output\",hlayer_output.shape)\n",
    "\n",
    "        final_logits = np.dot(hlayer_output, weights_ho) + bias_ho\n",
    "        # print(\"final_logits\",final_logits.shape)\n",
    "        final_output = sigmoid(final_logits)\n",
    "\n",
    "\n",
    "        tot_loss += abs(final_output[0] - y[i])\n",
    "\n",
    "        output_delta = (y[i] - final_output[0]) * final_output * (1 - final_output[0])\n",
    "        bias_ho += LEARNING_RATE * output_delta\n",
    "        bias_ih += LEARNING_RATE * (output_delta.dot(weights_ho.T) * hlayer_output * (1 - hlayer_output))\n",
    "\n",
    "        weights_ho += LEARNING_RATE * hlayer_output.T.dot(output_delta)\n",
    "        weights_ih += LEARNING_RATE * inp.T.dot(output_delta.dot(weights_ho.T) * hlayer_output * (1 - hlayer_output))\n",
    "        \n",
    "    print(f'Epoch {epoch} : loss {tot_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-14.98837889]] [[-0.24240941  2.22176463  1.49872937  2.03586551]]\n"
     ]
    }
   ],
   "source": [
    "print(bias_ho, bias_ih)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999901]]\n",
      "[[0.00016926]]\n"
     ]
    }
   ],
   "source": [
    "x_test1 = '1100110011'\n",
    "x_test2 = '1100110000'\n",
    "def test(x):\n",
    "    inp = np.reshape(np.array([int(char) for char in x]), (1, input_size))\n",
    "    return sigmoid(np.dot(sigmoid(np.dot(inp, weights_ih)+ bias_ih), weights_ho)+bias_ho)\n",
    "\n",
    "print(test(x_test1))\n",
    "print(test(x_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('weights_ih.npy', weights_ih)\n",
    "np.save('weights_ho.npy', weights_ho)\n",
    "np.save('bias_ih', bias_ih)\n",
    "np.save(\"bias_ho\", bias_ho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for i in range(len(x)):\n",
    "    y_p = 1 if test(x[i])>0.5 else 0\n",
    "    if y[i]!=y_p:\n",
    "        count+=1\n",
    "        print(x[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import activation, dataset, losses\n",
    "from model import *\n",
    "import numpy as np\n",
    "\n",
    "Dataset = dataset.Palindrome_Dataset(n_bits=10)\n",
    "x_train,y_train = Dataset.get_data(shuffle=True, biasing_factor=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_temp = x_train[:4]\n",
    "x_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'layer-1',\n",
       "  'weights': array([[0.52104617, 0.07185501, 0.3294009 , 0.6100576 ],\n",
       "         [0.57116313, 0.82440118, 0.8122118 , 0.50152226],\n",
       "         [0.83367467, 0.0648887 , 0.2445651 , 0.78049713],\n",
       "         [0.6211445 , 0.0257085 , 0.89328885, 0.16539725],\n",
       "         [0.3090168 , 0.17966955, 0.43676407, 0.75964128],\n",
       "         [0.81093867, 0.51780893, 0.71106392, 0.51255399],\n",
       "         [0.93381801, 0.89909393, 0.38772025, 0.74320917],\n",
       "         [0.5894768 , 0.95072858, 0.3834178 , 0.3517286 ],\n",
       "         [0.76471038, 0.87794411, 0.69724492, 0.95569085],\n",
       "         [0.36514249, 0.56562968, 0.45020004, 0.39085884]]),\n",
       "  'biases': array([0.89168599, 0.91266535, 0.1714021 , 0.531869  ]),\n",
       "  'activation': 'relu'},\n",
       " {'name': 'layer-2',\n",
       "  'weights': array([[0.53269987],\n",
       "         [0.66471686],\n",
       "         [0.54252673],\n",
       "         [0.53058214]]),\n",
       "  'biases': array([0.3132721]),\n",
       "  'activation': 'sigmoid'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Palindrome_Model(input_size=10, output_size=1,hidden_layer_sizes=[4],activation='sigmoid')\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99991837, 0.99263631, 0.99996002, 0.99948973])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(x_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_optimizer(lr=100,loss=\"mse\", optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200:   0%|          | 0/1024 [00:00<?, ?sample/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200:   0%|          | 1/1024 [00:00<00:57, 17.73sample/s, accuracy=96.9, loss=2.41e-14]\n",
      "Epoch 2/200:   0%|          | 1/1024 [00:00<00:47, 21.74sample/s, accuracy=3.12, loss=0.5]\n",
      "Epoch 3/200:   0%|          | 1/1024 [00:00<00:35, 28.79sample/s, accuracy=96.9, loss=0.5]\n",
      "Epoch 4/200:   0%|          | 1/1024 [00:00<00:33, 30.57sample/s, accuracy=96.9, loss=1.05e-9]\n",
      "Epoch 5/200:   0%|          | 1/1024 [00:00<00:32, 31.11sample/s, accuracy=96.9, loss=1.04e-9]\n",
      "Epoch 6/200:   0%|          | 1/1024 [00:00<00:34, 29.46sample/s, accuracy=96.9, loss=1.04e-9]\n",
      "Epoch 7/200:   0%|          | 1/1024 [00:00<00:35, 29.09sample/s, accuracy=96.9, loss=1.04e-9]\n",
      "Epoch 8/200:   0%|          | 1/1024 [00:00<00:37, 27.65sample/s, accuracy=96.9, loss=1.04e-9]\n",
      "Epoch 9/200:   0%|          | 1/1024 [00:00<00:35, 28.46sample/s, accuracy=96.9, loss=1.03e-9]\n",
      "Epoch 10/200:   0%|          | 1/1024 [00:00<00:35, 28.44sample/s, accuracy=96.9, loss=1.03e-9]\n",
      "Epoch 11/200:   0%|          | 1/1024 [00:00<00:39, 25.83sample/s, accuracy=96.9, loss=1.03e-9]\n",
      "Epoch 12/200:   0%|          | 1/1024 [00:00<00:40, 25.15sample/s, accuracy=96.9, loss=1.03e-9]\n",
      "Epoch 13/200:   0%|          | 1/1024 [00:00<00:36, 28.36sample/s, accuracy=96.9, loss=1.02e-9]\n",
      "Epoch 14/200:   0%|          | 1/1024 [00:00<00:41, 24.51sample/s, accuracy=96.9, loss=1.02e-9]\n",
      "Epoch 15/200:   0%|          | 1/1024 [00:00<00:40, 25.32sample/s, accuracy=96.9, loss=1.02e-9]\n",
      "Epoch 16/200:   0%|          | 1/1024 [00:00<00:38, 26.91sample/s, accuracy=96.9, loss=1.02e-9]\n",
      "Epoch 17/200:   0%|          | 1/1024 [00:00<00:35, 28.79sample/s, accuracy=96.9, loss=1.02e-9]\n",
      "Epoch 18/200:   0%|          | 1/1024 [00:00<00:35, 29.19sample/s, accuracy=96.9, loss=1.01e-9]\n",
      "Epoch 19/200:   0%|          | 1/1024 [00:00<00:36, 28.18sample/s, accuracy=96.9, loss=1.01e-9]\n",
      "Epoch 20/200:   0%|          | 1/1024 [00:00<00:35, 28.58sample/s, accuracy=96.9, loss=1.01e-9]\n",
      "Epoch 21/200:   0%|          | 1/1024 [00:00<00:35, 29.22sample/s, accuracy=96.9, loss=1.01e-9]\n",
      "Epoch 22/200:   0%|          | 1/1024 [00:00<00:38, 26.51sample/s, accuracy=96.9, loss=1e-9]\n",
      "Epoch 23/200:   0%|          | 1/1024 [00:00<00:40, 25.55sample/s, accuracy=96.9, loss=1e-9]\n",
      "Epoch 24/200:   0%|          | 1/1024 [00:00<00:41, 24.57sample/s, accuracy=96.9, loss=9.99e-10]\n",
      "Epoch 25/200:   0%|          | 1/1024 [00:00<00:42, 24.06sample/s, accuracy=96.9, loss=9.97e-10]\n",
      "Epoch 26/200:   0%|          | 1/1024 [00:00<00:38, 26.24sample/s, accuracy=96.9, loss=9.95e-10]\n",
      "Epoch 27/200:   0%|          | 1/1024 [00:00<00:42, 24.24sample/s, accuracy=3.12, loss=0.5]\n",
      "Epoch 28/200:   0%|          | 1/1024 [00:00<01:03, 16.00sample/s, accuracy=96.9, loss=0.5]\n",
      "Epoch 29/200:   0%|          | 1/1024 [00:00<01:19, 12.82sample/s, accuracy=96.9, loss=4.22e-5]\n",
      "Epoch 30/200:   0%|          | 1/1024 [00:00<00:35, 28.81sample/s, accuracy=96.9, loss=2.67e-5]\n",
      "Epoch 31/200:   0%|          | 1/1024 [00:00<00:34, 29.40sample/s, accuracy=96.9, loss=1.86e-5]\n",
      "Epoch 32/200:   0%|          | 1/1024 [00:00<00:39, 25.59sample/s, accuracy=96.9, loss=1.37e-5]\n",
      "Epoch 33/200:   0%|          | 1/1024 [00:00<00:34, 29.39sample/s, accuracy=96.9, loss=1.06e-5]\n",
      "Epoch 34/200:   0%|          | 1/1024 [00:00<00:36, 27.99sample/s, accuracy=96.9, loss=8.42e-6]\n",
      "Epoch 35/200:   0%|          | 1/1024 [00:00<00:33, 30.15sample/s, accuracy=96.9, loss=6.86e-6]\n",
      "Epoch 36/200:   0%|          | 1/1024 [00:00<00:35, 29.02sample/s, accuracy=96.9, loss=5.7e-6]\n",
      "Epoch 37/200:   0%|          | 1/1024 [00:00<00:36, 28.12sample/s, accuracy=96.9, loss=4.82e-6]\n",
      "Epoch 38/200:   0%|          | 1/1024 [00:00<00:34, 29.63sample/s, accuracy=96.9, loss=4.13e-6]\n",
      "Epoch 39/200:   0%|          | 1/1024 [00:00<00:38, 26.67sample/s, accuracy=96.9, loss=3.58e-6]\n",
      "Epoch 40/200:   0%|          | 1/1024 [00:00<00:37, 27.56sample/s, accuracy=96.9, loss=3.13e-6]\n",
      "Epoch 41/200:   0%|          | 1/1024 [00:00<00:38, 26.66sample/s, accuracy=96.9, loss=2.76e-6]\n",
      "Epoch 42/200:   0%|          | 1/1024 [00:00<00:38, 26.37sample/s, accuracy=96.9, loss=2.46e-6]\n",
      "Epoch 43/200:   0%|          | 1/1024 [00:00<00:39, 26.10sample/s, accuracy=96.9, loss=2.2e-6]\n",
      "Epoch 44/200:   0%|          | 1/1024 [00:00<00:40, 25.28sample/s, accuracy=96.9, loss=1.98e-6]\n",
      "Epoch 45/200:   0%|          | 1/1024 [00:00<00:39, 26.05sample/s, accuracy=3.12, loss=0.498]\n",
      "Epoch 46/200:   0%|          | 1/1024 [00:00<00:36, 27.82sample/s, accuracy=96.9, loss=0.5]\n",
      "Epoch 47/200:   0%|          | 1/1024 [00:00<00:38, 26.62sample/s, accuracy=96.9, loss=0.0375]\n",
      "Epoch 48/200:   0%|          | 1/1024 [00:00<00:36, 28.18sample/s, accuracy=96.9, loss=1.04e-7]\n",
      "Epoch 49/200:   0%|          | 1/1024 [00:00<00:42, 24.12sample/s, accuracy=96.9, loss=1.01e-7]\n",
      "Epoch 50/200:   0%|          | 1/1024 [00:00<00:42, 24.15sample/s, accuracy=96.9, loss=9.92e-8]\n",
      "Epoch 51/200:   0%|          | 1/1024 [00:00<00:38, 26.79sample/s, accuracy=96.9, loss=9.7e-8]\n",
      "Epoch 52/200:   0%|          | 1/1024 [00:00<00:41, 24.69sample/s, accuracy=96.9, loss=9.49e-8]\n",
      "Epoch 53/200:   0%|          | 1/1024 [00:00<00:36, 27.82sample/s, accuracy=96.9, loss=9.29e-8]\n",
      "Epoch 54/200:   0%|          | 1/1024 [00:00<00:46, 21.82sample/s, accuracy=96.9, loss=9.09e-8]\n",
      "Epoch 55/200:   0%|          | 1/1024 [00:00<00:40, 25.39sample/s, accuracy=96.9, loss=8.9e-8]\n",
      "Epoch 56/200:   0%|          | 1/1024 [00:00<00:36, 27.92sample/s, accuracy=96.9, loss=8.71e-8]\n",
      "Epoch 57/200:   0%|          | 1/1024 [00:00<00:37, 27.46sample/s, accuracy=96.9, loss=8.53e-8]\n",
      "Epoch 58/200:   0%|          | 1/1024 [00:00<00:38, 26.45sample/s, accuracy=96.9, loss=8.36e-8]\n",
      "Epoch 59/200:   0%|          | 1/1024 [00:00<00:38, 26.54sample/s, accuracy=96.9, loss=8.19e-8]\n",
      "Epoch 60/200:   0%|          | 1/1024 [00:00<00:38, 26.64sample/s, accuracy=96.9, loss=8.02e-8]\n",
      "Epoch 61/200:   0%|          | 1/1024 [00:00<00:35, 29.02sample/s, accuracy=96.9, loss=7.86e-8]\n",
      "Epoch 62/200:   0%|          | 1/1024 [00:00<00:38, 26.70sample/s, accuracy=96.9, loss=7.71e-8]\n",
      "Epoch 63/200:   0%|          | 1/1024 [00:00<00:36, 28.21sample/s, accuracy=96.9, loss=7.56e-8]\n",
      "Epoch 64/200:   0%|          | 1/1024 [00:00<00:39, 26.16sample/s, accuracy=96.9, loss=7.41e-8]\n",
      "Epoch 65/200:   0%|          | 1/1024 [00:00<00:44, 22.74sample/s, accuracy=96.9, loss=7.27e-8]\n",
      "Epoch 66/200:   0%|          | 1/1024 [00:00<00:37, 27.16sample/s, accuracy=96.9, loss=7.14e-8]\n",
      "Epoch 67/200:   0%|          | 1/1024 [00:00<00:44, 22.78sample/s, accuracy=96.9, loss=7e-8]\n",
      "Epoch 68/200:   0%|          | 1/1024 [00:00<00:33, 30.83sample/s, accuracy=96.9, loss=6.87e-8]\n",
      "Epoch 69/200:   0%|          | 1/1024 [00:00<00:33, 30.70sample/s, accuracy=96.9, loss=6.75e-8]\n",
      "Epoch 70/200:   0%|          | 1/1024 [00:00<00:43, 23.65sample/s, accuracy=96.9, loss=6.62e-8]\n",
      "Epoch 71/200:   0%|          | 1/1024 [00:00<00:35, 28.42sample/s, accuracy=96.9, loss=6.5e-8]\n",
      "Epoch 72/200:   0%|          | 1/1024 [00:00<00:40, 25.27sample/s, accuracy=96.9, loss=6.39e-8]\n",
      "Epoch 73/200:   0%|          | 1/1024 [00:00<00:34, 29.81sample/s, accuracy=96.9, loss=6.28e-8]\n",
      "Epoch 74/200:   0%|          | 1/1024 [00:00<00:35, 29.01sample/s, accuracy=96.9, loss=6.17e-8]\n",
      "Epoch 75/200:   0%|          | 1/1024 [00:00<00:35, 29.10sample/s, accuracy=96.9, loss=6.06e-8]\n",
      "Epoch 76/200:   0%|          | 1/1024 [00:00<00:33, 30.15sample/s, accuracy=96.9, loss=5.95e-8]\n",
      "Epoch 77/200:   0%|          | 1/1024 [00:00<00:33, 30.18sample/s, accuracy=96.9, loss=5.85e-8]\n",
      "Epoch 78/200:   0%|          | 1/1024 [00:00<00:34, 29.28sample/s, accuracy=96.9, loss=5.75e-8]\n",
      "Epoch 79/200:   0%|          | 1/1024 [00:00<00:36, 28.30sample/s, accuracy=96.9, loss=5.66e-8]\n",
      "Epoch 80/200:   0%|          | 1/1024 [00:00<00:38, 26.90sample/s, accuracy=96.9, loss=5.56e-8]\n",
      "Epoch 81/200:   0%|          | 1/1024 [00:00<00:42, 24.34sample/s, accuracy=96.9, loss=5.47e-8]\n",
      "Epoch 82/200:   0%|          | 1/1024 [00:00<00:38, 26.84sample/s, accuracy=3.12, loss=0.5]\n",
      "Epoch 83/200:   0%|          | 1/1024 [00:00<00:39, 26.09sample/s, accuracy=96.9, loss=0.5]\n",
      "Epoch 84/200:   0%|          | 1/1024 [00:00<00:38, 26.37sample/s, accuracy=96.9, loss=0.00201]\n",
      "Epoch 85/200:   0%|          | 1/1024 [00:00<00:35, 29.20sample/s, accuracy=96.9, loss=9.38e-5]\n",
      "Epoch 86/200:   0%|          | 1/1024 [00:00<00:37, 27.44sample/s, accuracy=3.12, loss=0.49]\n",
      "Epoch 87/200:   0%|          | 1/1024 [00:00<00:39, 25.71sample/s, accuracy=3.12, loss=0.5]\n",
      "Epoch 88/200:   0%|          | 1/1024 [00:00<00:34, 29.52sample/s, accuracy=96.9, loss=0.19]\n",
      "Epoch 89/200:   0%|          | 1/1024 [00:00<00:34, 29.67sample/s, accuracy=96.9, loss=8.3e-13]\n",
      "Epoch 90/200:   0%|          | 1/1024 [00:00<00:34, 29.77sample/s, accuracy=96.9, loss=8.3e-13]\n",
      "Epoch 91/200:   0%|          | 1/1024 [00:00<00:36, 28.10sample/s, accuracy=96.9, loss=8.3e-13]\n",
      "Epoch 92/200:   0%|          | 1/1024 [00:00<00:40, 25.17sample/s, accuracy=96.9, loss=8.3e-13]\n",
      "Epoch 93/200:   0%|          | 1/1024 [00:00<00:34, 29.92sample/s, accuracy=96.9, loss=8.3e-13]\n",
      "Epoch 94/200:   0%|          | 1/1024 [00:00<00:33, 30.28sample/s, accuracy=96.9, loss=8.3e-13]\n",
      "Epoch 95/200:   0%|          | 1/1024 [00:00<01:16, 13.39sample/s, accuracy=96.9, loss=8.3e-13]\n",
      "Epoch 96/200:   0%|          | 1/1024 [00:00<00:49, 20.56sample/s, accuracy=96.9, loss=8.3e-13]\n",
      "Epoch 97/200:   0%|          | 1/1024 [00:00<00:34, 29.66sample/s, accuracy=96.9, loss=8.3e-13]\n",
      "Epoch 98/200:   0%|          | 1/1024 [00:00<00:43, 23.46sample/s, accuracy=96.9, loss=8.3e-13]\n",
      "Epoch 99/200:   0%|          | 1/1024 [00:00<00:33, 30.38sample/s, accuracy=96.9, loss=8.3e-13]\n",
      "Epoch 100/200:   0%|          | 1/1024 [00:00<00:36, 27.73sample/s, accuracy=96.9, loss=8.3e-13]\n",
      "Epoch 101/200:   0%|          | 1/1024 [00:00<00:36, 28.15sample/s, accuracy=96.9, loss=8.3e-13]\n",
      "Epoch 102/200:   0%|          | 1/1024 [00:00<00:34, 29.60sample/s, accuracy=96.9, loss=8.3e-13]\n",
      "Epoch 103/200:   0%|          | 1/1024 [00:00<00:34, 29.32sample/s, accuracy=96.9, loss=8.3e-13]\n",
      "Epoch 104/200:   0%|          | 1/1024 [00:00<00:35, 29.06sample/s, accuracy=96.9, loss=8.3e-13]\n",
      "Epoch 105/200:   0%|          | 1/1024 [00:00<00:34, 29.73sample/s, accuracy=96.9, loss=8.29e-13]\n",
      "Epoch 106/200:   0%|          | 1/1024 [00:00<00:33, 30.12sample/s, accuracy=96.9, loss=8.29e-13]\n",
      "Epoch 107/200:   0%|          | 1/1024 [00:00<00:32, 31.09sample/s, accuracy=96.9, loss=8.29e-13]\n",
      "Epoch 108/200:   0%|          | 1/1024 [00:00<00:33, 30.94sample/s, accuracy=96.9, loss=8.29e-13]\n",
      "Epoch 109/200:   0%|          | 1/1024 [00:00<00:34, 29.61sample/s, accuracy=96.9, loss=8.29e-13]\n",
      "Epoch 110/200:   0%|          | 1/1024 [00:00<00:35, 29.11sample/s, accuracy=96.9, loss=8.29e-13]\n",
      "Epoch 111/200:   0%|          | 1/1024 [00:00<00:37, 27.09sample/s, accuracy=96.9, loss=8.29e-13]\n",
      "Epoch 112/200:   0%|          | 1/1024 [00:00<00:34, 29.56sample/s, accuracy=96.9, loss=8.29e-13]\n",
      "Epoch 113/200:   0%|          | 1/1024 [00:00<00:34, 29.57sample/s, accuracy=96.9, loss=8.29e-13]\n",
      "Epoch 114/200:   0%|          | 1/1024 [00:00<00:39, 26.09sample/s, accuracy=96.9, loss=8.29e-13]\n",
      "Epoch 115/200:   0%|          | 1/1024 [00:00<00:44, 22.98sample/s, accuracy=96.9, loss=8.29e-13]\n",
      "Epoch 116/200:   0%|          | 1/1024 [00:00<00:40, 24.97sample/s, accuracy=96.9, loss=8.29e-13]\n",
      "Epoch 117/200:   0%|          | 1/1024 [00:00<00:34, 29.41sample/s, accuracy=96.9, loss=8.29e-13]\n",
      "Epoch 118/200:   0%|          | 1/1024 [00:00<00:36, 27.97sample/s, accuracy=96.9, loss=8.29e-13]\n",
      "Epoch 119/200:   0%|          | 1/1024 [00:00<00:38, 26.46sample/s, accuracy=96.9, loss=8.29e-13]\n",
      "Epoch 120/200:   0%|          | 1/1024 [00:00<00:49, 20.87sample/s, accuracy=96.9, loss=8.29e-13]\n",
      "Epoch 121/200:   0%|          | 1/1024 [00:00<00:46, 22.19sample/s, accuracy=96.9, loss=8.29e-13]\n",
      "Epoch 122/200:   0%|          | 1/1024 [00:00<00:38, 26.49sample/s, accuracy=96.9, loss=8.29e-13]\n",
      "Epoch 123/200:   0%|          | 1/1024 [00:00<00:41, 24.42sample/s, accuracy=96.9, loss=8.28e-13]\n",
      "Epoch 124/200:   0%|          | 1/1024 [00:00<00:44, 22.80sample/s, accuracy=96.9, loss=8.28e-13]\n",
      "Epoch 125/200:   0%|          | 1/1024 [00:00<00:37, 27.06sample/s, accuracy=96.9, loss=8.28e-13]\n",
      "Epoch 126/200:   0%|          | 1/1024 [00:00<00:43, 23.32sample/s, accuracy=96.9, loss=8.28e-13]\n",
      "Epoch 127/200:   0%|          | 1/1024 [00:00<00:36, 27.87sample/s, accuracy=96.9, loss=8.28e-13]\n",
      "Epoch 128/200:   0%|          | 1/1024 [00:00<00:46, 21.89sample/s, accuracy=96.9, loss=8.28e-13]\n",
      "Epoch 129/200:   0%|          | 1/1024 [00:00<00:45, 22.30sample/s, accuracy=96.9, loss=8.28e-13]\n",
      "Epoch 130/200:   0%|          | 1/1024 [00:00<00:39, 25.65sample/s, accuracy=96.9, loss=8.28e-13]\n",
      "Epoch 131/200:   0%|          | 1/1024 [00:00<00:36, 27.70sample/s, accuracy=96.9, loss=8.28e-13]\n",
      "Epoch 132/200:   0%|          | 1/1024 [00:00<00:36, 28.37sample/s, accuracy=96.9, loss=8.28e-13]\n",
      "Epoch 133/200:   0%|          | 1/1024 [00:00<00:36, 27.76sample/s, accuracy=96.9, loss=8.28e-13]\n",
      "Epoch 134/200:   0%|          | 1/1024 [00:00<00:37, 27.22sample/s, accuracy=96.9, loss=8.28e-13]\n",
      "Epoch 135/200:   0%|          | 1/1024 [00:00<00:38, 26.43sample/s, accuracy=96.9, loss=8.28e-13]\n",
      "Epoch 136/200:   0%|          | 1/1024 [00:00<00:37, 27.07sample/s, accuracy=96.9, loss=8.28e-13]\n",
      "Epoch 137/200:   0%|          | 1/1024 [00:00<00:35, 28.95sample/s, accuracy=96.9, loss=8.28e-13]\n",
      "Epoch 138/200:   0%|          | 1/1024 [00:00<00:42, 24.16sample/s, accuracy=96.9, loss=8.28e-13]\n",
      "Epoch 139/200:   0%|          | 1/1024 [00:00<00:44, 22.81sample/s, accuracy=96.9, loss=8.28e-13]\n",
      "Epoch 140/200:   0%|          | 1/1024 [00:00<00:41, 24.57sample/s, accuracy=96.9, loss=8.28e-13]\n",
      "Epoch 141/200:   0%|          | 1/1024 [00:00<00:42, 23.94sample/s, accuracy=96.9, loss=8.28e-13]\n",
      "Epoch 142/200:   0%|          | 1/1024 [00:00<00:59, 17.31sample/s, accuracy=96.9, loss=8.27e-13]\n",
      "Epoch 143/200:   0%|          | 1/1024 [00:00<01:21, 12.60sample/s, accuracy=96.9, loss=8.27e-13]\n",
      "Epoch 144/200:   0%|          | 1/1024 [00:00<00:48, 21.25sample/s, accuracy=96.9, loss=8.27e-13]\n",
      "Epoch 145/200:   0%|          | 1/1024 [00:00<00:46, 21.77sample/s, accuracy=96.9, loss=8.27e-13]\n",
      "Epoch 146/200:   0%|          | 1/1024 [00:00<00:40, 25.33sample/s, accuracy=96.9, loss=8.27e-13]\n",
      "Epoch 147/200:   0%|          | 1/1024 [00:00<00:39, 25.91sample/s, accuracy=96.9, loss=8.27e-13]\n",
      "Epoch 148/200:   0%|          | 1/1024 [00:00<00:35, 29.05sample/s, accuracy=96.9, loss=8.27e-13]\n",
      "Epoch 149/200:   0%|          | 1/1024 [00:00<00:34, 29.54sample/s, accuracy=96.9, loss=8.27e-13]\n",
      "Epoch 150/200:   0%|          | 1/1024 [00:00<00:51, 19.85sample/s, accuracy=96.9, loss=8.27e-13]\n",
      "Epoch 151/200:   0%|          | 1/1024 [00:00<01:00, 16.91sample/s, accuracy=96.9, loss=8.27e-13]\n",
      "Epoch 152/200:   0%|          | 1/1024 [00:00<01:05, 15.69sample/s, accuracy=96.9, loss=8.27e-13]\n",
      "Epoch 153/200:   0%|          | 1/1024 [00:00<00:51, 19.97sample/s, accuracy=96.9, loss=8.27e-13]\n",
      "Epoch 154/200:   0%|          | 1/1024 [00:00<00:39, 25.69sample/s, accuracy=96.9, loss=8.27e-13]\n",
      "Epoch 155/200:   0%|          | 1/1024 [00:00<00:38, 26.83sample/s, accuracy=96.9, loss=8.27e-13]\n",
      "Epoch 156/200:   0%|          | 1/1024 [00:00<00:42, 23.89sample/s, accuracy=96.9, loss=8.27e-13]\n",
      "Epoch 157/200:   0%|          | 1/1024 [00:00<00:38, 26.49sample/s, accuracy=96.9, loss=8.27e-13]\n",
      "Epoch 158/200:   0%|          | 1/1024 [00:00<00:37, 26.95sample/s, accuracy=96.9, loss=8.27e-13]\n",
      "Epoch 159/200:   0%|          | 1/1024 [00:00<00:39, 25.94sample/s, accuracy=96.9, loss=8.27e-13]\n",
      "Epoch 160/200:   0%|          | 1/1024 [00:00<00:32, 31.61sample/s, accuracy=96.9, loss=8.27e-13]\n",
      "Epoch 161/200:   0%|          | 1/1024 [00:00<00:35, 28.45sample/s, accuracy=96.9, loss=8.26e-13]\n",
      "Epoch 162/200:   0%|          | 1/1024 [00:00<00:33, 30.35sample/s, accuracy=96.9, loss=8.26e-13]\n",
      "Epoch 163/200:   0%|          | 1/1024 [00:00<00:36, 28.16sample/s, accuracy=96.9, loss=8.26e-13]\n",
      "Epoch 164/200:   0%|          | 1/1024 [00:00<00:45, 22.61sample/s, accuracy=96.9, loss=8.26e-13]\n",
      "Epoch 165/200:   0%|          | 1/1024 [00:00<00:43, 23.53sample/s, accuracy=96.9, loss=8.26e-13]\n",
      "Epoch 166/200:   0%|          | 1/1024 [00:00<01:22, 12.43sample/s, accuracy=96.9, loss=8.26e-13]\n",
      "Epoch 167/200:   0%|          | 1/1024 [00:00<00:58, 17.48sample/s, accuracy=96.9, loss=8.26e-13]\n",
      "Epoch 168/200:   0%|          | 1/1024 [00:00<00:45, 22.69sample/s, accuracy=96.9, loss=8.26e-13]\n",
      "Epoch 169/200:   0%|          | 1/1024 [00:00<00:49, 20.84sample/s, accuracy=96.9, loss=8.26e-13]\n",
      "Epoch 170/200:   0%|          | 1/1024 [00:00<01:06, 15.34sample/s, accuracy=96.9, loss=8.26e-13]\n",
      "Epoch 171/200:   0%|          | 1/1024 [00:00<00:42, 23.87sample/s, accuracy=96.9, loss=8.26e-13]\n",
      "Epoch 172/200:   0%|          | 1/1024 [00:00<00:35, 28.70sample/s, accuracy=96.9, loss=8.26e-13]\n",
      "Epoch 173/200:   0%|          | 1/1024 [00:00<00:35, 29.12sample/s, accuracy=3.12, loss=0.5]\n",
      "Epoch 174/200:   0%|          | 1/1024 [00:00<00:36, 28.39sample/s, accuracy=96.9, loss=0.5]\n",
      "Epoch 175/200:   0%|          | 1/1024 [00:00<00:42, 24.31sample/s, accuracy=96.9, loss=3.58e-8]\n",
      "Epoch 176/200:   0%|          | 1/1024 [00:00<00:50, 20.22sample/s, accuracy=96.9, loss=3.53e-8]\n",
      "Epoch 177/200:   0%|          | 1/1024 [00:00<00:46, 21.82sample/s, accuracy=96.9, loss=3.49e-8]\n",
      "Epoch 178/200:   0%|          | 1/1024 [00:00<00:48, 21.05sample/s, accuracy=96.9, loss=3.44e-8]\n",
      "Epoch 179/200:   0%|          | 1/1024 [00:00<00:45, 22.56sample/s, accuracy=96.9, loss=3.4e-8]\n",
      "Epoch 180/200:   0%|          | 1/1024 [00:00<00:43, 23.51sample/s, accuracy=96.9, loss=3.35e-8]\n",
      "Epoch 181/200:   0%|          | 1/1024 [00:00<00:50, 20.06sample/s, accuracy=96.9, loss=3.31e-8]\n",
      "Epoch 182/200:   0%|          | 1/1024 [00:00<00:41, 24.88sample/s, accuracy=96.9, loss=3.27e-8]\n",
      "Epoch 183/200:   0%|          | 1/1024 [00:00<00:45, 22.36sample/s, accuracy=96.9, loss=3.23e-8]\n",
      "Epoch 184/200:   0%|          | 1/1024 [00:00<00:41, 24.55sample/s, accuracy=96.9, loss=3.18e-8]\n",
      "Epoch 185/200:   0%|          | 1/1024 [00:00<00:43, 23.46sample/s, accuracy=96.9, loss=3.14e-8]\n",
      "Epoch 186/200:   0%|          | 1/1024 [00:00<00:39, 25.68sample/s, accuracy=96.9, loss=3.11e-8]\n",
      "Epoch 187/200:   0%|          | 1/1024 [00:00<00:44, 22.99sample/s, accuracy=96.9, loss=3.07e-8]\n",
      "Epoch 188/200:   0%|          | 1/1024 [00:00<00:48, 21.24sample/s, accuracy=96.9, loss=3.03e-8]\n",
      "Epoch 189/200:   0%|          | 1/1024 [00:00<00:46, 22.03sample/s, accuracy=96.9, loss=2.99e-8]\n",
      "Epoch 190/200:   0%|          | 1/1024 [00:00<01:21, 12.48sample/s, accuracy=96.9, loss=2.96e-8]\n",
      "Epoch 191/200:   0%|          | 1/1024 [00:00<00:46, 21.82sample/s, accuracy=96.9, loss=2.92e-8]\n",
      "Epoch 192/200:   0%|          | 1/1024 [00:00<00:45, 22.45sample/s, accuracy=96.9, loss=2.88e-8]\n",
      "Epoch 193/200:   0%|          | 1/1024 [00:00<00:46, 22.13sample/s, accuracy=96.9, loss=2.85e-8]\n",
      "Epoch 194/200:   0%|          | 1/1024 [00:00<00:44, 22.93sample/s, accuracy=3.12, loss=0.5]\n",
      "Epoch 195/200:   0%|          | 1/1024 [00:00<00:50, 20.11sample/s, accuracy=96.9, loss=0.5]\n",
      "Epoch 196/200:   0%|          | 1/1024 [00:00<00:50, 20.35sample/s, accuracy=96.9, loss=0.0011]\n",
      "Epoch 197/200:   0%|          | 1/1024 [00:00<00:52, 19.63sample/s, accuracy=96.9, loss=0.000113]\n",
      "Epoch 198/200:   0%|          | 1/1024 [00:00<00:47, 21.49sample/s, accuracy=96.9, loss=5.37e-5]\n",
      "Epoch 199/200:   0%|          | 1/1024 [00:00<00:51, 19.70sample/s, accuracy=96.9, loss=3.21e-5]\n",
      "Epoch 200/200:   0%|          | 1/1024 [00:00<00:53, 18.95sample/s, accuracy=96.9, loss=2.16e-5]\n"
     ]
    }
   ],
   "source": [
    "accuracies, losses = model.train(x_train,y_train,epochs=200,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(\u001b[43mx\u001b[49m)\n\u001b[1;32m      2\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(y_pred, \u001b[38;5;241m1e-12\u001b[39m, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1e-12\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(y \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(y_pred) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m y) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m (y_pred)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "# y_pred = model(x)\n",
    "# y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)\n",
    "# -np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - (y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m temp_x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m[:\u001b[38;5;241m48\u001b[39m]\n\u001b[1;32m      2\u001b[0m temp_y \u001b[38;5;241m=\u001b[39m y[:\u001b[38;5;241m48\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(temp_x, temp_y)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "temp_x = x[:48]\n",
    "temp_y = y[:48]\n",
    "print(temp_x, temp_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resolving Bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import dataset\n",
    "from model import Palindrome_Model\n",
    "import numpy as np\n",
    "#np.random.seed(7)\n",
    "\n",
    "Dataset = dataset.Palindrome_Dataset(n_bits=10)\n",
    "x,y = Dataset.get_data(shuffle=True, biasing_factor=20)\n",
    "model = Palindrome_Model(input_size=10, output_size=1,hidden_layer_sizes=[4],activation='sigmoid')\n",
    "model.set_optimizer(lr=0.005,loss=\"bce\", optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 1 1 1 1 0 1] [1]\n",
      "[1 1 1 1 1 1 1 1 0 1] [0]\n",
      "[0. 0. 0. 0. 1. 1. 0. 0. 0. 0.] [1]\n"
     ]
    }
   ],
   "source": [
    "temp = [np.array([1,0,1,1,1,1,1,1,0,1]), np.array([1,1,1,1,1,1,1,1,0,1]), np.array([0,0,0,0,1,1,0,0,0,0], dtype=float)]\n",
    "y_temp = np.array([[1], [0], [1]])\n",
    "\n",
    "\n",
    "for x,y in zip(temp,y_temp):\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97403188],\n",
       "       [0.91646151],\n",
       "       [0.84928304]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs [array([1, 0, 1, 1, 1, 1, 1, 1, 0, 1]), array([1.11147228, 0.        , 0.80965221, 2.29672724]), array([0.97403188])]\n",
      "Label [1]\n",
      "Predicted [0.97403188]\n",
      "Loss Grad: [-1.02666045]\n",
      "Act Grad: [0.02529378]\n",
      "Loss Grad: [-0.026181    0.00943286  0.0268221  -0.04267106]\n",
      "Act Grad: 1.0\n",
      "Outputs [array([1, 1, 1, 1, 1, 1, 1, 1, 0, 1]), array([0.        , 0.        , 0.9913761 , 2.34475633]), array([0.91646151])]\n",
      "Label [0]\n",
      "Predicted [0.91646151]\n",
      "Loss Grad: [11.97053005]\n",
      "Act Grad: [0.07655981]\n",
      "Loss Grad: [ 0.92397416 -0.33290256 -0.94659987  1.50593803]\n",
      "Act Grad: 0.0\n",
      "Outputs [array([0., 0., 0., 0., 1., 1., 0., 0., 0., 0.]), array([2.41982488, 1.27662388, 0.        , 0.11367569]), array([0.84928304])]\n",
      "Label [1]\n",
      "Predicted [0.84928304]\n",
      "Loss Grad: [-1.17746376]\n",
      "Act Grad: [0.12800136]\n",
      "Loss Grad: [-0.15195245  0.05474759  0.15567337 -0.2476595 ]\n",
      "Act Grad: 1.0\n"
     ]
    }
   ],
   "source": [
    "model.backward(temp,y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02909946,  0.        ,  0.02171657, -0.09800379])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([-0.026181, 0.00943286,  0.0268221,  -0.04267106])*np.array([1.11147228, 0.        , 0.80965221, 2.29672724])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02909946, -0.        , -0.0211975 , -0.06013062],\n",
       "       [ 0.01048436,  0.        ,  0.00763734,  0.02166471],\n",
       "       [ 0.02981202,  0.        ,  0.02171657,  0.06160305],\n",
       "       [-0.0474277 , -0.        , -0.03454872, -0.09800379]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.outer(np.array([-0.026181, 0.00943286,  0.0268221,  -0.04267106]),np.array([1.11147228,      0.  , 0.80965221, 2.29672724]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.008727  ,  0.00314429,  0.0089407 , -0.01422369],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [-0.008727  ,  0.00314429,  0.0089407 , -0.01422369],\n",
       "        [-0.008727  ,  0.00314429,  0.0089407 , -0.01422369],\n",
       "        [-0.05937782,  0.02139349,  0.06083182, -0.09677685],\n",
       "        [-0.05937782,  0.02139349,  0.06083182, -0.09677685],\n",
       "        [-0.008727  ,  0.00314429,  0.0089407 , -0.01422369],\n",
       "        [-0.008727  ,  0.00314429,  0.0089407 , -0.01422369],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [-0.008727  ,  0.00314429,  0.0089407 , -0.01422369]]),\n",
       " array([[-0.1311905 ],\n",
       "        [-0.06413629],\n",
       "        [ 0.2958443 ],\n",
       "        [ 0.69070146]])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.05937782,  0.02139349,  0.06083182, -0.09677685]),\n",
       " array([0.24659214])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bias_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_palindromes = np.array([np.reshape(np.array([int(char) for char in s]), (10,)) for s in Dataset.palindromes])\n",
    "y_palindromes = np.array([1]*32)\n",
    "\n",
    "x_non_palindrome = np.array([np.reshape(np.array([int(char) for char in s]), (10,)) for s in Dataset.non_palindromes])\n",
    "y_non_palindrome = np.array([0]*len(x_non_palindrome))\n",
    "temp_x = np.concatenate((temp_x,x_palindromes))\n",
    "temp_y = np.concatenate((temp_y, y_palindromes))\n",
    "accuracies, losses = model.train(x_palindromes,y_palindromes,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_optimizer(lr=0.005,loss=\"mse\")\n",
    "accuracies, losses = model.train(temp_x,temp_y,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_palindromes)\n",
    "accuracy_metric(predictions, y_palindromes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_non_palindrome)\n",
    "# print(predictions)\n",
    "accuracy_metric(predictions, y_non_palindrome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch  Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'layer-1',\n",
       "  'weights': array([[ 0.93264673,  0.97459294, -0.12848894,  0.58921758],\n",
       "         [-0.61967231, -0.26271703, -0.26330762, -0.93215773],\n",
       "         [ 0.19383003, -1.06437594,  0.40447317, -0.24151142],\n",
       "         [-0.44076791, -0.58080072, -0.21027712, -0.61963486],\n",
       "         [-0.21196891, -0.25251799, -0.36971576, -0.07134193],\n",
       "         [-0.01738654,  0.49040936, -0.42500098, -0.1270861 ],\n",
       "         [-0.82970687,  0.77321702,  0.29156391, -0.54454293],\n",
       "         [ 0.42967041, -0.02238592, -0.17803028,  0.43492721],\n",
       "         [ 0.39676217, -0.09361975,  0.69012828,  0.31874703],\n",
       "         [ 0.4822464 ,  0.3571908 ,  0.47563312,  0.07272139]]),\n",
       "  'biases': array([-0.73642481, -0.58666945, -0.33631103,  0.80275304]),\n",
       "  'activation': 'relu'},\n",
       " {'name': 'layer-2',\n",
       "  'weights': array([[ 0.59679054],\n",
       "         [-0.80576963],\n",
       "         [ 0.08172214],\n",
       "         [-0.04323667]]),\n",
       "  'biases': array([2.17619008]),\n",
       "  'activation': 'sigmoid'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 1 1 1 1 0 1] 1\n",
      "[1 1 1 1 1 1 1 1 0 1] 0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.88995978],\n",
       "       [0.89793558]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(model,X:np.ndarray, y:np.ndarray, k_folds:int = 4, epochs_per_fold:int = 25):\n",
    "        fold_size = len(X) // k_folds\n",
    "        accuracies_fold = []\n",
    "        accuracies=[]\n",
    "        losses = []\n",
    "        for i in range(k_folds):\n",
    "            start, end = i * fold_size, (i + 1) * fold_size\n",
    "            X_test_fold, y_test_fold = X[start:end], y[start:end]\n",
    "            X_train_fold = np.concatenate([X[:start], X[end:]])\n",
    "            y_train_fold = np.concatenate([y[:start], y[end:]])\n",
    "            print(f\"Training Fold {i+1}\\n\")\n",
    "            accuracy, loss = model.train(X_train_fold, y_train_fold, epochs_per_fold)\n",
    "            accuracies.extend(accuracy)\n",
    "            losses.extend(loss)\n",
    "            print(\"\\n\"+10*\"----\"+\"\\n\")\n",
    "            predictions = model.predict(X_test_fold)\n",
    "            accuracies_fold.append(accuracy_metric(predictions, y_test_fold))\n",
    "        return accuracies_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = k_fold_cross_validation(model, x, y, epochs_per_fold=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x)\n",
    "accuracy_metric(predictions, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(y)):\n",
    "    if predictions[i]==1:\n",
    "        count+=1\n",
    "        # print(x[i], y[i], predictions[i])\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1664"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-13 23:04:12.338627: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-13 23:04:12.341532: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-13 23:04:12.378010: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-13 23:04:12.378049: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-13 23:04:12.378944: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-13 23:04:12.385559: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-13 23:04:12.386113: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-13 23:04:13.396302: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 4)                 44        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 49 (196.00 Byte)\n",
      "Trainable params: 49 (196.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-13 23:04:14.371571: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-13 23:04:14.371946: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential([\n",
    "    layers.InputLayer(input_shape=(10,)),          # Input layer with size 10\n",
    "    layers.Dense(4, activation='relu'),          # Hidden layer with size 4 and linear activation\n",
    "    layers.Dense(1, activation='sigmoid')           # Output layer with size 1 and sigmoid activation\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.0)\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer,loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "208/208 [==============================] - 0s 944us/step - loss: 0.2419 - accuracy: 0.5980\n",
      "Epoch 2/100\n",
      "208/208 [==============================] - 0s 928us/step - loss: 0.2322 - accuracy: 0.6250\n",
      "Epoch 3/100\n",
      "208/208 [==============================] - 0s 873us/step - loss: 0.2205 - accuracy: 0.6508\n",
      "Epoch 4/100\n",
      "208/208 [==============================] - 0s 903us/step - loss: 0.2045 - accuracy: 0.6701\n",
      "Epoch 5/100\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.1936 - accuracy: 0.6911\n",
      "Epoch 6/100\n",
      "208/208 [==============================] - 0s 957us/step - loss: 0.1863 - accuracy: 0.7206\n",
      "Epoch 7/100\n",
      "208/208 [==============================] - 0s 976us/step - loss: 0.1821 - accuracy: 0.7121\n",
      "Epoch 8/100\n",
      "208/208 [==============================] - 0s 955us/step - loss: 0.1782 - accuracy: 0.7236\n",
      "Epoch 9/100\n",
      "208/208 [==============================] - 0s 886us/step - loss: 0.1758 - accuracy: 0.7236\n",
      "Epoch 10/100\n",
      "208/208 [==============================] - 0s 915us/step - loss: 0.1733 - accuracy: 0.7200\n",
      "Epoch 11/100\n",
      "208/208 [==============================] - 0s 900us/step - loss: 0.1712 - accuracy: 0.7290\n",
      "Epoch 12/100\n",
      "208/208 [==============================] - 0s 902us/step - loss: 0.1698 - accuracy: 0.7224\n",
      "Epoch 13/100\n",
      "208/208 [==============================] - 0s 917us/step - loss: 0.1676 - accuracy: 0.7284\n",
      "Epoch 14/100\n",
      "208/208 [==============================] - 0s 927us/step - loss: 0.1668 - accuracy: 0.7314\n",
      "Epoch 15/100\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.1652 - accuracy: 0.7260\n",
      "Epoch 16/100\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.1640 - accuracy: 0.7338\n",
      "Epoch 17/100\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.1631 - accuracy: 0.7308\n",
      "Epoch 18/100\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.1622 - accuracy: 0.7338\n",
      "Epoch 19/100\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.1613 - accuracy: 0.7362\n",
      "Epoch 20/100\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.1604 - accuracy: 0.7350\n",
      "Epoch 21/100\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.1599 - accuracy: 0.7368\n",
      "Epoch 22/100\n",
      "208/208 [==============================] - 0s 944us/step - loss: 0.1593 - accuracy: 0.7410\n",
      "Epoch 23/100\n",
      "208/208 [==============================] - 0s 898us/step - loss: 0.1583 - accuracy: 0.7428\n",
      "Epoch 24/100\n",
      "208/208 [==============================] - 0s 912us/step - loss: 0.1571 - accuracy: 0.7410\n",
      "Epoch 25/100\n",
      "208/208 [==============================] - 0s 959us/step - loss: 0.1566 - accuracy: 0.7440\n",
      "Epoch 26/100\n",
      "208/208 [==============================] - 0s 948us/step - loss: 0.1557 - accuracy: 0.7428\n",
      "Epoch 27/100\n",
      "208/208 [==============================] - 0s 958us/step - loss: 0.1521 - accuracy: 0.7578\n",
      "Epoch 28/100\n",
      "208/208 [==============================] - 0s 936us/step - loss: 0.1465 - accuracy: 0.7843\n",
      "Epoch 29/100\n",
      "208/208 [==============================] - 0s 952us/step - loss: 0.1388 - accuracy: 0.8131\n",
      "Epoch 30/100\n",
      "208/208 [==============================] - 0s 956us/step - loss: 0.1225 - accuracy: 0.8335\n",
      "Epoch 31/100\n",
      "208/208 [==============================] - 0s 949us/step - loss: 0.1005 - accuracy: 0.9026\n",
      "Epoch 32/100\n",
      "208/208 [==============================] - 0s 946us/step - loss: 0.0864 - accuracy: 0.9225\n",
      "Epoch 33/100\n",
      "208/208 [==============================] - 0s 966us/step - loss: 0.0740 - accuracy: 0.9447\n",
      "Epoch 34/100\n",
      "208/208 [==============================] - 0s 994us/step - loss: 0.0604 - accuracy: 0.9585\n",
      "Epoch 35/100\n",
      "208/208 [==============================] - 0s 975us/step - loss: 0.0521 - accuracy: 0.9651\n",
      "Epoch 36/100\n",
      "208/208 [==============================] - 0s 960us/step - loss: 0.0461 - accuracy: 0.9675\n",
      "Epoch 37/100\n",
      "208/208 [==============================] - 0s 954us/step - loss: 0.0414 - accuracy: 0.9772\n",
      "Epoch 38/100\n",
      "208/208 [==============================] - 0s 940us/step - loss: 0.0362 - accuracy: 0.9820\n",
      "Epoch 39/100\n",
      "208/208 [==============================] - 0s 955us/step - loss: 0.0328 - accuracy: 0.9838\n",
      "Epoch 40/100\n",
      "208/208 [==============================] - 0s 936us/step - loss: 0.0296 - accuracy: 0.9880\n",
      "Epoch 41/100\n",
      "208/208 [==============================] - 0s 948us/step - loss: 0.0270 - accuracy: 0.9910\n",
      "Epoch 42/100\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.0250 - accuracy: 0.9898\n",
      "Epoch 43/100\n",
      "208/208 [==============================] - 0s 937us/step - loss: 0.0226 - accuracy: 0.9910\n",
      "Epoch 44/100\n",
      "208/208 [==============================] - 0s 924us/step - loss: 0.0209 - accuracy: 0.9928\n",
      "Epoch 45/100\n",
      "208/208 [==============================] - 0s 926us/step - loss: 0.0195 - accuracy: 0.9922\n",
      "Epoch 46/100\n",
      "208/208 [==============================] - 0s 919us/step - loss: 0.0182 - accuracy: 0.9934\n",
      "Epoch 47/100\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.0166 - accuracy: 0.9952\n",
      "Epoch 48/100\n",
      "208/208 [==============================] - 0s 973us/step - loss: 0.0156 - accuracy: 0.9958\n",
      "Epoch 49/100\n",
      "208/208 [==============================] - 0s 936us/step - loss: 0.0147 - accuracy: 0.9964\n",
      "Epoch 50/100\n",
      "208/208 [==============================] - 0s 964us/step - loss: 0.0135 - accuracy: 0.9952\n",
      "Epoch 51/100\n",
      "208/208 [==============================] - 0s 916us/step - loss: 0.0127 - accuracy: 0.9976\n",
      "Epoch 52/100\n",
      "208/208 [==============================] - 0s 952us/step - loss: 0.0123 - accuracy: 0.9970\n",
      "Epoch 53/100\n",
      "208/208 [==============================] - 0s 961us/step - loss: 0.0115 - accuracy: 0.9976\n",
      "Epoch 54/100\n",
      "208/208 [==============================] - 0s 957us/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "208/208 [==============================] - 0s 976us/step - loss: 0.0103 - accuracy: 0.9982\n",
      "Epoch 56/100\n",
      "208/208 [==============================] - 0s 907us/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "208/208 [==============================] - 0s 996us/step - loss: 0.0094 - accuracy: 0.9994\n",
      "Epoch 58/100\n",
      "208/208 [==============================] - 0s 904us/step - loss: 0.0088 - accuracy: 0.9994\n",
      "Epoch 59/100\n",
      "208/208 [==============================] - 0s 940us/step - loss: 0.0085 - accuracy: 0.9994\n",
      "Epoch 60/100\n",
      "208/208 [==============================] - 0s 964us/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "208/208 [==============================] - 0s 907us/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "208/208 [==============================] - 0s 905us/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "208/208 [==============================] - 0s 934us/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "208/208 [==============================] - 0s 920us/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "208/208 [==============================] - 0s 934us/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "208/208 [==============================] - 0s 991us/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "208/208 [==============================] - 0s 944us/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "208/208 [==============================] - 0s 936us/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "208/208 [==============================] - 0s 937us/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "208/208 [==============================] - 0s 955us/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "208/208 [==============================] - 0s 948us/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "208/208 [==============================] - 0s 937us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "208/208 [==============================] - 0s 995us/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "208/208 [==============================] - 0s 996us/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "208/208 [==============================] - 0s 934us/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "208/208 [==============================] - 0s 936us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "208/208 [==============================] - 0s 934us/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "208/208 [==============================] - 0s 949us/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "208/208 [==============================] - 0s 949us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "208/208 [==============================] - 0s 990us/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "208/208 [==============================] - 0s 937us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "208/208 [==============================] - 0s 906us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "208/208 [==============================] - 0s 943us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "208/208 [==============================] - 0s 930us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "208/208 [==============================] - 0s 940us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "208/208 [==============================] - 0s 957us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "208/208 [==============================] - 0s 969us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "208/208 [==============================] - 0s 956us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "208/208 [==============================] - 0s 986us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "208/208 [==============================] - 0s 949us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "208/208 [==============================] - 0s 972us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "208/208 [==============================] - 0s 960us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "208/208 [==============================] - 0s 935us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "208/208 [==============================] - 0s 936us/step - loss: 0.0025 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f8a9824bfd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=100, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_palindrome(arr):\n",
    "    if np.array_equal(arr, np.flip(arr)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for inp in x:\n",
    "    y_pred.append(is_palindrome(inp))\n",
    "\n",
    "count = 0\n",
    "for i in range(len(y)):\n",
    "    if not y_pred[i]==y[i]:\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sem8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
