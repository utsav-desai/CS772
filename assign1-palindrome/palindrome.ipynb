{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from faker import Faker\n",
    "import random\n",
    "np.random.seed()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "palindromes = []\n",
    "for i in range(1024):\n",
    "    binary_string = format(i, '010b')\n",
    "    x.append(binary_string)\n",
    "    if binary_string == binary_string[::-1]:\n",
    "        palindromes.append(binary_string)\n",
    "x = np.array(x)\n",
    "palindromes = np.array(palindromes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['0000000000', '0000110000', '0001001000', '0001111000',\n",
       "       '0010000100', '0010110100', '0011001100', '0011111100',\n",
       "       '0100000010', '0100110010', '0101001010', '0101111010',\n",
       "       '0110000110', '0110110110', '0111001110', '0111111110',\n",
       "       '1000000001', '1000110001', '1001001001', '1001111001',\n",
       "       '1010000101', '1010110101', '1011001101', '1011111101',\n",
       "       '1100000011', '1100110011', '1101001011', '1101111011',\n",
       "       '1110000111', '1110110111', '1111001111', '1111111111'],\n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(palindromes))\n",
    "palindromes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['0110110110', '0101001010', '0111111110', '1101111011',\n",
       "       '1110000111', '1001111001', '0110000110', '1110110111',\n",
       "       '0101111010', '1011111101', '0000110000', '1111111111',\n",
       "       '1010000101', '1100110011', '1011001101', '1001001001',\n",
       "       '1010110101', '0111001110', '1111001111', '0010000100',\n",
       "       '1000110001', '0001001000', '0001111000', '0100000010',\n",
       "       '1100000011', '0000000000', '1000000001', '0011001100',\n",
       "       '0100110010', '0010110100', '0011111100', '1101001011'],\n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(palindromes)\n",
    "print(len(palindromes))\n",
    "palindromes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "# for i in range(5):\n",
    "#     x = np.concatenate((x, palindromes))\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for binary_string in x:\n",
    "    y.append(binary_string == binary_string[::-1])\n",
    "y = np.array(y)\n",
    "permutation_index = np.random.permutation(len(x))\n",
    "x = x[permutation_index]\n",
    "y = y[permutation_index]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-1 * x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.array([1, 2, -3])\n",
    "# b = np.array([2, 3, 4])\n",
    "# a*b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 10000\n",
    "\n",
    "input_size = 10\n",
    "hidden_layer_size = 2\n",
    "output_size = 1\n",
    "\n",
    "weights_ih = np.load('weights/weights_ih2.npy')\n",
    "# weights_ih = np.random.rand(input_size, hidden_layer_size)\n",
    "weights_ho = np.random.rand(hidden_layer_size, output_size)\n",
    "bias_ih = np.load('weights/bias_ih2.npy')\n",
    "# bias_ih = np.random.rand(1, hidden_layer_size)\n",
    "bias_ho = np.random.rand(1, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_ih = np.load('weights_ih2.npy')\n",
    "# weights_ho = np.load('weights_ho2.npy')\n",
    "# bias_ih = np.load('bias_ih2.npy')\n",
    "# bias_ho = np.load('bias_ho2.npy')\n",
    "# weights_ih = weights_ih[:, [1, 3]]\n",
    "# bias_ih = bias_ih[:, [1, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : loss [21.72319047]\n",
      "Epoch 1 : loss [12.65511803]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 : loss [12.49180084]\n",
      "Epoch 3 : loss [12.4323052]\n",
      "Epoch 4 : loss [12.40040131]\n",
      "Epoch 5 : loss [12.37925625]\n",
      "Epoch 6 : loss [12.36309877]\n",
      "Epoch 7 : loss [12.34943684]\n",
      "Epoch 8 : loss [12.33701959]\n",
      "Epoch 9 : loss [12.32514556]\n",
      "Epoch 10 : loss [12.31338535]\n",
      "Epoch 11 : loss [12.30145671]\n",
      "Epoch 12 : loss [12.28916357]\n",
      "Epoch 13 : loss [12.27636446]\n",
      "Epoch 14 : loss [12.26295561]\n",
      "Epoch 15 : loss [12.24886138]\n",
      "Epoch 16 : loss [12.23402844]\n",
      "Epoch 17 : loss [12.21842116]\n",
      "Epoch 18 : loss [12.20201665]\n",
      "Epoch 19 : loss [12.18479853]\n",
      "Epoch 20 : loss [12.1667486]\n",
      "Epoch 21 : loss [12.14783656]\n",
      "Epoch 22 : loss [12.12800826]\n",
      "Epoch 23 : loss [12.10717342]\n",
      "Epoch 24 : loss [12.08519394]\n",
      "Epoch 25 : loss [12.06187343]\n",
      "Epoch 26 : loss [12.03694821]\n",
      "Epoch 27 : loss [12.01007912]\n",
      "Epoch 28 : loss [11.98084273]\n",
      "Epoch 29 : loss [11.94872055]\n",
      "Epoch 30 : loss [11.9130841]\n",
      "Epoch 31 : loss [11.87317447]\n",
      "Epoch 32 : loss [11.82807441]\n",
      "Epoch 33 : loss [11.7766722]\n",
      "Epoch 34 : loss [11.71761661]\n",
      "Epoch 35 : loss [11.64926444]\n",
      "Epoch 36 : loss [11.56962596]\n",
      "Epoch 37 : loss [11.47632126]\n",
      "Epoch 38 : loss [11.36657559]\n",
      "Epoch 39 : loss [11.23730643]\n",
      "Epoch 40 : loss [11.08538917]\n",
      "Epoch 41 : loss [10.90821509]\n",
      "Epoch 42 : loss [10.70462358]\n",
      "Epoch 43 : loss [10.47610418]\n",
      "Epoch 44 : loss [10.22777258]\n",
      "Epoch 45 : loss [9.96825619]\n",
      "Epoch 46 : loss [9.70789321]\n",
      "Epoch 47 : loss [9.45583435]\n",
      "Epoch 48 : loss [9.21770922]\n",
      "Epoch 49 : loss [8.99510375]\n",
      "Epoch 50 : loss [8.78483039]\n",
      "Epoch 51 : loss [8.56159903]\n",
      "Epoch 52 : loss [8.34868838]\n",
      "Epoch 53 : loss [8.14272512]\n",
      "Epoch 54 : loss [7.94127461]\n",
      "Epoch 55 : loss [7.74257471]\n",
      "Epoch 56 : loss [7.54517454]\n",
      "Epoch 57 : loss [7.34770229]\n",
      "Epoch 58 : loss [7.14882442]\n",
      "Epoch 59 : loss [6.94734895]\n",
      "Epoch 60 : loss [6.74238673]\n",
      "Epoch 61 : loss [6.53349139]\n",
      "Epoch 62 : loss [6.32072793]\n",
      "Epoch 63 : loss [6.10466417]\n",
      "Epoch 64 : loss [5.88632744]\n",
      "Epoch 65 : loss [5.66718415]\n",
      "Epoch 66 : loss [5.44915676]\n",
      "Epoch 67 : loss [5.23461873]\n",
      "Epoch 68 : loss [5.02627539]\n",
      "Epoch 69 : loss [4.82688817]\n",
      "Epoch 70 : loss [4.63890212]\n",
      "Epoch 71 : loss [4.46411164]\n",
      "Epoch 72 : loss [4.30348434]\n",
      "Epoch 73 : loss [4.15717576]\n",
      "Epoch 74 : loss [4.02468266]\n",
      "Epoch 75 : loss [3.90505174]\n",
      "Epoch 76 : loss [3.79707851]\n",
      "Epoch 77 : loss [3.69946437]\n",
      "Epoch 78 : loss [3.61092539]\n",
      "Epoch 79 : loss [3.53025927]\n",
      "Epoch 80 : loss [3.45638061]\n",
      "Epoch 81 : loss [3.3883344]\n",
      "Epoch 82 : loss [3.32529563]\n",
      "Epoch 83 : loss [3.26656088]\n",
      "Epoch 84 : loss [3.21153583]\n",
      "Epoch 85 : loss [3.15972145]\n",
      "Epoch 86 : loss [3.11070021]\n",
      "Epoch 87 : loss [3.06412338]\n",
      "Epoch 88 : loss [3.01969967]\n",
      "Epoch 89 : loss [2.9771854]\n",
      "Epoch 90 : loss [2.93637619]\n",
      "Epoch 91 : loss [2.89709992]\n",
      "Epoch 92 : loss [2.85921095]\n",
      "Epoch 93 : loss [2.82258533]\n",
      "Epoch 94 : loss [2.78711693]\n",
      "Epoch 95 : loss [2.75271423]\n",
      "Epoch 96 : loss [2.71929775]\n",
      "Epoch 97 : loss [2.68679798]\n",
      "Epoch 98 : loss [2.65515364]\n",
      "Epoch 99 : loss [2.62431039]\n",
      "Epoch 100 : loss [2.65060888]\n",
      "Epoch 101 : loss [2.63049153]\n",
      "Epoch 102 : loss [2.59986985]\n",
      "Epoch 103 : loss [2.56922554]\n",
      "Epoch 104 : loss [2.53903858]\n",
      "Epoch 105 : loss [2.50944352]\n",
      "Epoch 106 : loss [2.48046081]\n",
      "Epoch 107 : loss [2.45207504]\n",
      "Epoch 108 : loss [2.42426072]\n",
      "Epoch 109 : loss [2.39699072]\n",
      "Epoch 110 : loss [2.37023894]\n",
      "Epoch 111 : loss [2.34398102]\n",
      "Epoch 112 : loss [2.31819454]\n",
      "Epoch 113 : loss [2.29285889]\n",
      "Epoch 114 : loss [2.26795517]\n",
      "Epoch 115 : loss [2.243466]\n",
      "Epoch 116 : loss [2.21937544]\n",
      "Epoch 117 : loss [2.19566882]\n",
      "Epoch 118 : loss [2.17233264]\n",
      "Epoch 119 : loss [2.14935444]\n",
      "Epoch 120 : loss [2.12672274]\n",
      "Epoch 121 : loss [2.10442686]\n",
      "Epoch 122 : loss [2.08245695]\n",
      "Epoch 123 : loss [2.06080379]\n",
      "Epoch 124 : loss [2.03945883]\n",
      "Epoch 125 : loss [2.01841404]\n",
      "Epoch 126 : loss [1.99766189]\n",
      "Epoch 127 : loss [1.97719532]\n",
      "Epoch 128 : loss [1.95700764]\n",
      "Epoch 129 : loss [1.93709253]\n",
      "Epoch 130 : loss [1.91744401]\n",
      "Epoch 131 : loss [1.89805637]\n",
      "Epoch 132 : loss [1.87892419]\n",
      "Epoch 133 : loss [1.86004227]\n",
      "Epoch 134 : loss [1.84140565]\n",
      "Epoch 135 : loss [1.82300958]\n",
      "Epoch 136 : loss [1.80484948]\n",
      "Epoch 137 : loss [1.78692096]\n",
      "Epoch 138 : loss [1.7692198]\n",
      "Epoch 139 : loss [1.75174192]\n",
      "Epoch 140 : loss [1.73448339]\n",
      "Epoch 141 : loss [1.71744042]\n",
      "Epoch 142 : loss [1.70060934]\n",
      "Epoch 143 : loss [1.68398661]\n",
      "Epoch 144 : loss [1.6675688]\n",
      "Epoch 145 : loss [1.65135258]\n",
      "Epoch 146 : loss [1.63533475]\n",
      "Epoch 147 : loss [1.61951218]\n",
      "Epoch 148 : loss [1.60388186]\n",
      "Epoch 149 : loss [1.58844085]\n",
      "Epoch 150 : loss [1.64873129]\n",
      "Epoch 151 : loss [1.65451311]\n",
      "Epoch 152 : loss [1.6394859]\n",
      "Epoch 153 : loss [1.62302749]\n",
      "Epoch 154 : loss [1.60629127]\n",
      "Epoch 155 : loss [1.58954898]\n",
      "Epoch 156 : loss [1.57290682]\n",
      "Epoch 157 : loss [1.55641332]\n",
      "Epoch 158 : loss [1.54009016]\n",
      "Epoch 159 : loss [1.52394611]\n",
      "Epoch 160 : loss [1.50798396]\n",
      "Epoch 161 : loss [1.49220377]\n",
      "Epoch 162 : loss [1.47660448]\n",
      "Epoch 163 : loss [1.46118458]\n",
      "Epoch 164 : loss [1.44594241]\n",
      "Epoch 165 : loss [1.43087628]\n",
      "Epoch 166 : loss [1.41598449]\n",
      "Epoch 167 : loss [1.4012654]\n",
      "Epoch 168 : loss [1.38671737]\n",
      "Epoch 169 : loss [1.37233878]\n",
      "Epoch 170 : loss [1.35812803]\n",
      "Epoch 171 : loss [1.34408357]\n",
      "Epoch 172 : loss [1.33020383]\n",
      "Epoch 173 : loss [1.31648727]\n",
      "Epoch 174 : loss [1.3029324]\n",
      "Epoch 175 : loss [1.28953771]\n",
      "Epoch 176 : loss [1.27630173]\n",
      "Epoch 177 : loss [1.263223]\n",
      "Epoch 178 : loss [1.25030009]\n",
      "Epoch 179 : loss [1.23753159]\n",
      "Epoch 180 : loss [1.22491608]\n",
      "Epoch 181 : loss [1.21245218]\n",
      "Epoch 182 : loss [1.20013852]\n",
      "Epoch 183 : loss [1.18797374]\n",
      "Epoch 184 : loss [1.17595649]\n",
      "Epoch 185 : loss [1.16408542]\n",
      "Epoch 186 : loss [1.15235921]\n",
      "Epoch 187 : loss [1.14077652]\n",
      "Epoch 188 : loss [1.12933605]\n",
      "Epoch 189 : loss [1.11803647]\n",
      "Epoch 190 : loss [1.10687647]\n",
      "Epoch 191 : loss [1.09585475]\n",
      "Epoch 192 : loss [1.08496999]\n",
      "Epoch 193 : loss [1.07422089]\n",
      "Epoch 194 : loss [1.06360615]\n",
      "Epoch 195 : loss [1.05312445]\n",
      "Epoch 196 : loss [1.04277448]\n",
      "Epoch 197 : loss [1.03255495]\n",
      "Epoch 198 : loss [1.02246453]\n",
      "Epoch 199 : loss [1.01250192]\n",
      "Epoch 200 : loss [1.07674936]\n",
      "Epoch 201 : loss [1.10088694]\n",
      "Epoch 202 : loss [1.09148619]\n",
      "Epoch 203 : loss [1.08049163]\n",
      "Epoch 204 : loss [1.06879506]\n",
      "Epoch 205 : loss [1.056807]\n",
      "Epoch 206 : loss [1.04473491]\n",
      "Epoch 207 : loss [1.03268279]\n",
      "Epoch 208 : loss [1.02070616]\n",
      "Epoch 209 : loss [1.00883718]\n",
      "Epoch 210 : loss [0.99709583]\n",
      "Epoch 211 : loss [0.98549506]\n",
      "Epoch 212 : loss [0.97404346]\n",
      "Epoch 213 : loss [0.96274667]\n",
      "Epoch 214 : loss [0.95160825]\n",
      "Epoch 215 : loss [0.94063027]\n",
      "Epoch 216 : loss [0.92981369]\n",
      "Epoch 217 : loss [0.91915866]\n",
      "Epoch 218 : loss [0.90866471]\n",
      "Epoch 219 : loss [0.89833094]\n",
      "Epoch 220 : loss [0.88815611]\n",
      "Epoch 221 : loss [0.87813873]\n",
      "Epoch 222 : loss [0.86827714]\n",
      "Epoch 223 : loss [0.85856956]\n",
      "Epoch 224 : loss [0.8490141]\n",
      "Epoch 225 : loss [0.8396088]\n",
      "Epoch 226 : loss [0.83035167]\n",
      "Epoch 227 : loss [0.82124067]\n",
      "Epoch 228 : loss [0.81227374]\n",
      "Epoch 229 : loss [0.8034488]\n",
      "Epoch 230 : loss [0.79476376]\n",
      "Epoch 231 : loss [0.78621653]\n",
      "Epoch 232 : loss [0.77780502]\n",
      "Epoch 233 : loss [0.76952712]\n",
      "Epoch 234 : loss [0.76138075]\n",
      "Epoch 235 : loss [0.75336382]\n",
      "Epoch 236 : loss [0.74547425]\n",
      "Epoch 237 : loss [0.73770998]\n",
      "Epoch 238 : loss [0.73006894]\n",
      "Epoch 239 : loss [0.7225491]\n",
      "Epoch 240 : loss [0.71514842]\n",
      "Epoch 241 : loss [0.70786488]\n",
      "Epoch 242 : loss [0.70069649]\n",
      "Epoch 243 : loss [0.69364126]\n",
      "Epoch 244 : loss [0.68669724]\n",
      "Epoch 245 : loss [0.67986248]\n",
      "Epoch 246 : loss [0.67313507]\n",
      "Epoch 247 : loss [0.66651309]\n",
      "Epoch 248 : loss [0.65999468]\n",
      "Epoch 249 : loss [0.65357797]\n",
      "Epoch 250 : loss [0.70432697]\n",
      "Epoch 251 : loss [0.73585988]\n",
      "Epoch 252 : loss [0.73117878]\n",
      "Epoch 253 : loss [0.72417784]\n",
      "Epoch 254 : loss [0.71595887]\n",
      "Epoch 255 : loss [0.7071306]\n",
      "Epoch 256 : loss [0.69799901]\n",
      "Epoch 257 : loss [0.68875383]\n",
      "Epoch 258 : loss [0.67951839]\n",
      "Epoch 259 : loss [0.67037326]\n",
      "Epoch 260 : loss [0.66137016]\n",
      "Epoch 261 : loss [0.65254119]\n",
      "Epoch 262 : loss [0.64390504]\n",
      "Epoch 263 : loss [0.63547122]\n",
      "Epoch 264 : loss [0.62724303]\n",
      "Epoch 265 : loss [0.61921964]\n",
      "Epoch 266 : loss [0.61139753]\n",
      "Epoch 267 : loss [0.60377155]\n",
      "Epoch 268 : loss [0.59633558]\n",
      "Epoch 269 : loss [0.58908303]\n",
      "Epoch 270 : loss [0.58200715]\n",
      "Epoch 271 : loss [0.57510124]\n",
      "Epoch 272 : loss [0.56835876]\n",
      "Epoch 273 : loss [0.56177342]\n",
      "Epoch 274 : loss [0.55533919]\n",
      "Epoch 275 : loss [0.54905037]\n",
      "Epoch 276 : loss [0.54290153]\n",
      "Epoch 277 : loss [0.53688755]\n",
      "Epoch 278 : loss [0.53100356]\n",
      "Epoch 279 : loss [0.52524498]\n",
      "Epoch 280 : loss [0.51960747]\n",
      "Epoch 281 : loss [0.51408692]\n",
      "Epoch 282 : loss [0.50867941]\n",
      "Epoch 283 : loss [0.50338126]\n",
      "Epoch 284 : loss [0.49818895]\n",
      "Epoch 285 : loss [0.49309913]\n",
      "Epoch 286 : loss [0.48810861]\n",
      "Epoch 287 : loss [0.48321438]\n",
      "Epoch 288 : loss [0.47841352]\n",
      "Epoch 289 : loss [0.47370328]\n",
      "Epoch 290 : loss [0.46908101]\n",
      "Epoch 291 : loss [0.46454417]\n",
      "Epoch 292 : loss [0.46009034]\n",
      "Epoch 293 : loss [0.45571721]\n",
      "Epoch 294 : loss [0.45142252]\n",
      "Epoch 295 : loss [0.44720416]\n",
      "Epoch 296 : loss [0.44306005]\n",
      "Epoch 297 : loss [0.43898822]\n",
      "Epoch 298 : loss [0.43498676]\n",
      "Epoch 299 : loss [0.43105384]\n",
      "Epoch 300 : loss [0.46337162]\n",
      "Epoch 301 : loss [0.48530491]\n",
      "Epoch 302 : loss [0.48249351]\n",
      "Epoch 303 : loss [0.47814166]\n",
      "Epoch 304 : loss [0.47284655]\n",
      "Epoch 305 : loss [0.46708271]\n",
      "Epoch 306 : loss [0.46110455]\n",
      "Epoch 307 : loss [0.45506162]\n",
      "Epoch 308 : loss [0.44904834]\n",
      "Epoch 309 : loss [0.44312373]\n",
      "Epoch 310 : loss [0.43732342]\n",
      "Epoch 311 : loss [0.4316675]\n",
      "Epoch 312 : loss [0.42616581]\n",
      "Epoch 313 : loss [0.42082146]\n",
      "Epoch 314 : loss [0.41563323]\n",
      "Epoch 315 : loss [0.41059723]\n",
      "Epoch 316 : loss [0.40570803]\n",
      "Epoch 317 : loss [0.40095938]\n",
      "Epoch 318 : loss [0.39634474]\n",
      "Epoch 319 : loss [0.39185757]\n",
      "Epoch 320 : loss [0.38749153]\n",
      "Epoch 321 : loss [0.38324055]\n",
      "Epoch 322 : loss [0.37909892]\n",
      "Epoch 323 : loss [0.3750613]\n",
      "Epoch 324 : loss [0.37112271]\n",
      "Epoch 325 : loss [0.36727852]\n",
      "Epoch 326 : loss [0.36352443]\n",
      "Epoch 327 : loss [0.35985645]\n",
      "Epoch 328 : loss [0.35627085]\n",
      "Epoch 329 : loss [0.35276418]\n",
      "Epoch 330 : loss [0.34933322]\n",
      "Epoch 331 : loss [0.34597496]\n",
      "Epoch 332 : loss [0.34268659]\n",
      "Epoch 333 : loss [0.33946546]\n",
      "Epoch 334 : loss [0.33630911]\n",
      "Epoch 335 : loss [0.33321521]\n",
      "Epoch 336 : loss [0.33018157]\n",
      "Epoch 337 : loss [0.32720613]\n",
      "Epoch 338 : loss [0.32428693]\n",
      "Epoch 339 : loss [0.32142214]\n",
      "Epoch 340 : loss [0.31860999]\n",
      "Epoch 341 : loss [0.31584884]\n",
      "Epoch 342 : loss [0.31313711]\n",
      "Epoch 343 : loss [0.31047329]\n",
      "Epoch 344 : loss [0.30785596]\n",
      "Epoch 345 : loss [0.30528375]\n",
      "Epoch 346 : loss [0.30275538]\n",
      "Epoch 347 : loss [0.30026959]\n",
      "Epoch 348 : loss [0.29782521]\n",
      "Epoch 349 : loss [0.29542109]\n",
      "Epoch 350 : loss [0.31500863]\n",
      "Epoch 351 : loss [0.32689485]\n",
      "Epoch 352 : loss [0.32462903]\n",
      "Epoch 353 : loss [0.32179083]\n",
      "Epoch 354 : loss [0.3184947]\n",
      "Epoch 355 : loss [0.31500856]\n",
      "Epoch 356 : loss [0.31146246]\n",
      "Epoch 357 : loss [0.30791985]\n",
      "Epoch 358 : loss [0.30441638]\n",
      "Epoch 359 : loss [0.30097263]\n",
      "Epoch 360 : loss [0.29760026]\n",
      "Epoch 361 : loss [0.29430545]\n",
      "Epoch 362 : loss [0.29109093]\n",
      "Epoch 363 : loss [0.28795716]\n",
      "Epoch 364 : loss [0.28490317]\n",
      "Epoch 365 : loss [0.28192705]\n",
      "Epoch 366 : loss [0.27902634]\n",
      "Epoch 367 : loss [0.27619829]\n",
      "Epoch 368 : loss [0.27344]\n",
      "Epoch 369 : loss [0.27074857]\n",
      "Epoch 370 : loss [0.26812114]\n",
      "Epoch 371 : loss [0.26555495]\n",
      "Epoch 372 : loss [0.26304737]\n",
      "Epoch 373 : loss [0.26059589]\n",
      "Epoch 374 : loss [0.25819816]\n",
      "Epoch 375 : loss [0.25585194]\n",
      "Epoch 376 : loss [0.25355516]\n",
      "Epoch 377 : loss [0.25130584]\n",
      "Epoch 378 : loss [0.24910214]\n",
      "Epoch 379 : loss [0.24694234]\n",
      "Epoch 380 : loss [0.2448248]\n",
      "Epoch 381 : loss [0.24274798]\n",
      "Epoch 382 : loss [0.24071045]\n",
      "Epoch 383 : loss [0.23871085]\n",
      "Epoch 384 : loss [0.23674788]\n",
      "Epoch 385 : loss [0.23482033]\n",
      "Epoch 386 : loss [0.23292705]\n",
      "Epoch 387 : loss [0.23106695]\n",
      "Epoch 388 : loss [0.22923899]\n",
      "Epoch 389 : loss [0.2274422]\n",
      "Epoch 390 : loss [0.22567563]\n",
      "Epoch 391 : loss [0.22393839]\n",
      "Epoch 392 : loss [0.22222965]\n",
      "Epoch 393 : loss [0.22054857]\n",
      "Epoch 394 : loss [0.2188944]\n",
      "Epoch 395 : loss [0.2172664]\n",
      "Epoch 396 : loss [0.21566385]\n",
      "Epoch 397 : loss [0.21408608]\n",
      "Epoch 398 : loss [0.21253244]\n",
      "Epoch 399 : loss [0.2110023]\n",
      "Epoch 400 : loss [0.22336646]\n",
      "Epoch 401 : loss [0.22984344]\n",
      "Epoch 402 : loss [0.22823238]\n",
      "Epoch 403 : loss [0.2263839]\n",
      "Epoch 404 : loss [0.22430043]\n",
      "Epoch 405 : loss [0.2221408]\n",
      "Epoch 406 : loss [0.21996927]\n",
      "Epoch 407 : loss [0.21781293]\n",
      "Epoch 408 : loss [0.21568541]\n",
      "Epoch 409 : loss [0.21359387]\n",
      "Epoch 410 : loss [0.21154206]\n",
      "Epoch 411 : loss [0.20953175]\n",
      "Epoch 412 : loss [0.20756359]\n",
      "Epoch 413 : loss [0.20563748]\n",
      "Epoch 414 : loss [0.20375286]\n",
      "Epoch 415 : loss [0.20190886]\n",
      "Epoch 416 : loss [0.20010442]\n",
      "Epoch 417 : loss [0.19833836]\n",
      "Epoch 418 : loss [0.19660944]\n",
      "Epoch 419 : loss [0.19491641]\n",
      "Epoch 420 : loss [0.193258]\n",
      "Epoch 421 : loss [0.191633]\n",
      "Epoch 422 : loss [0.19004021]\n",
      "Epoch 423 : loss [0.18847847]\n",
      "Epoch 424 : loss [0.1869467]\n",
      "Epoch 425 : loss [0.18544384]\n",
      "Epoch 426 : loss [0.18396889]\n",
      "Epoch 427 : loss [0.1825209]\n",
      "Epoch 428 : loss [0.18109896]\n",
      "Epoch 429 : loss [0.17970221]\n",
      "Epoch 430 : loss [0.17832984]\n",
      "Epoch 431 : loss [0.17698107]\n",
      "Epoch 432 : loss [0.17565516]\n",
      "Epoch 433 : loss [0.17435141]\n",
      "Epoch 434 : loss [0.17306915]\n",
      "Epoch 435 : loss [0.17180774]\n",
      "Epoch 436 : loss [0.17056658]\n",
      "Epoch 437 : loss [0.16934509]\n",
      "Epoch 438 : loss [0.16814272]\n",
      "Epoch 439 : loss [0.16695894]\n",
      "Epoch 440 : loss [0.16579324]\n",
      "Epoch 441 : loss [0.16464513]\n",
      "Epoch 442 : loss [0.16351417]\n",
      "Epoch 443 : loss [0.16239989]\n",
      "Epoch 444 : loss [0.16130188]\n",
      "Epoch 445 : loss [0.16021973]\n",
      "Epoch 446 : loss [0.15915304]\n",
      "Epoch 447 : loss [0.15810143]\n",
      "Epoch 448 : loss [0.15706455]\n",
      "Epoch 449 : loss [0.15604204]\n",
      "Epoch 450 : loss [0.16429706]\n",
      "Epoch 451 : loss [0.16809879]\n",
      "Epoch 452 : loss [0.16699233]\n",
      "Epoch 453 : loss [0.16575306]\n",
      "Epoch 454 : loss [0.16437812]\n",
      "Epoch 455 : loss [0.1629669]\n",
      "Epoch 456 : loss [0.16155507]\n",
      "Epoch 457 : loss [0.16015631]\n",
      "Epoch 458 : loss [0.15877685]\n",
      "Epoch 459 : loss [0.15741967]\n",
      "Epoch 460 : loss [0.15608612]\n",
      "Epoch 461 : loss [0.15477674]\n",
      "Epoch 462 : loss [0.15349162]\n",
      "Epoch 463 : loss [0.15223058]\n",
      "Epoch 464 : loss [0.15099326]\n",
      "Epoch 465 : loss [0.1497792]\n",
      "Epoch 466 : loss [0.14858788]\n",
      "Epoch 467 : loss [0.1474187]\n",
      "Epoch 468 : loss [0.14627107]\n",
      "Epoch 469 : loss [0.14514436]\n",
      "Epoch 470 : loss [0.14403796]\n",
      "Epoch 471 : loss [0.14295125]\n",
      "Epoch 472 : loss [0.14188363]\n",
      "Epoch 473 : loss [0.14083453]\n",
      "Epoch 474 : loss [0.13980337]\n",
      "Epoch 475 : loss [0.1387896]\n",
      "Epoch 476 : loss [0.13779271]\n",
      "Epoch 477 : loss [0.13681219]\n",
      "Epoch 478 : loss [0.13584755]\n",
      "Epoch 479 : loss [0.13489833]\n",
      "Epoch 480 : loss [0.1339641]\n",
      "Epoch 481 : loss [0.13304442]\n",
      "Epoch 482 : loss [0.13213889]\n",
      "Epoch 483 : loss [0.13124712]\n",
      "Epoch 484 : loss [0.13036874]\n",
      "Epoch 485 : loss [0.12950338]\n",
      "Epoch 486 : loss [0.12865071]\n",
      "Epoch 487 : loss [0.1278104]\n",
      "Epoch 488 : loss [0.12698214]\n",
      "Epoch 489 : loss [0.12616561]\n",
      "Epoch 490 : loss [0.12536053]\n",
      "Epoch 491 : loss [0.12456662]\n",
      "Epoch 492 : loss [0.12378361]\n",
      "Epoch 493 : loss [0.12301125]\n",
      "Epoch 494 : loss [0.12224927]\n",
      "Epoch 495 : loss [0.12149744]\n",
      "Epoch 496 : loss [0.12075554]\n",
      "Epoch 497 : loss [0.12002333]\n",
      "Epoch 498 : loss [0.1193006]\n",
      "Epoch 499 : loss [0.11858714]\n",
      "Epoch 500 : loss [0.12437654]\n",
      "Epoch 501 : loss [0.1267848]\n",
      "Epoch 502 : loss [0.1260112]\n",
      "Epoch 503 : loss [0.12514883]\n",
      "Epoch 504 : loss [0.12420052]\n",
      "Epoch 505 : loss [0.12323147]\n",
      "Epoch 506 : loss [0.12226399]\n",
      "Epoch 507 : loss [0.12130616]\n",
      "Epoch 508 : loss [0.12036136]\n",
      "Epoch 509 : loss [0.11943106]\n",
      "Epoch 510 : loss [0.11851581]\n",
      "Epoch 511 : loss [0.11761577]\n",
      "Epoch 512 : loss [0.11673089]\n",
      "Epoch 513 : loss [0.11586098]\n",
      "Epoch 514 : loss [0.11500582]\n",
      "Epoch 515 : loss [0.11416513]\n",
      "Epoch 516 : loss [0.1133386]\n",
      "Epoch 517 : loss [0.1125259]\n",
      "Epoch 518 : loss [0.1117267]\n",
      "Epoch 519 : loss [0.11094066]\n",
      "Epoch 520 : loss [0.11016743]\n",
      "Epoch 521 : loss [0.10940667]\n",
      "Epoch 522 : loss [0.10865805]\n",
      "Epoch 523 : loss [0.10792123]\n",
      "Epoch 524 : loss [0.1071959]\n",
      "Epoch 525 : loss [0.10648175]\n",
      "Epoch 526 : loss [0.10577846]\n",
      "Epoch 527 : loss [0.10508576]\n",
      "Epoch 528 : loss [0.10440335]\n",
      "Epoch 529 : loss [0.10373097]\n",
      "Epoch 530 : loss [0.10306836]\n",
      "Epoch 531 : loss [0.10241526]\n",
      "Epoch 532 : loss [0.10177143]\n",
      "Epoch 533 : loss [0.10113664]\n",
      "Epoch 534 : loss [0.10051067]\n",
      "Epoch 535 : loss [0.0998933]\n",
      "Epoch 536 : loss [0.09928432]\n",
      "Epoch 537 : loss [0.09868353]\n",
      "Epoch 538 : loss [0.09809074]\n",
      "Epoch 539 : loss [0.09750576]\n",
      "Epoch 540 : loss [0.09692842]\n",
      "Epoch 541 : loss [0.09635854]\n",
      "Epoch 542 : loss [0.09579595]\n",
      "Epoch 543 : loss [0.09524049]\n",
      "Epoch 544 : loss [0.09469201]\n",
      "Epoch 545 : loss [0.09415036]\n",
      "Epoch 546 : loss [0.09361539]\n",
      "Epoch 547 : loss [0.09308696]\n",
      "Epoch 548 : loss [0.09256494]\n",
      "Epoch 549 : loss [0.09204918]\n",
      "Epoch 550 : loss [0.09627201]\n",
      "Epoch 551 : loss [0.09789785]\n",
      "Epoch 552 : loss [0.09734106]\n",
      "Epoch 553 : loss [0.09672017]\n",
      "Epoch 554 : loss [0.096041]\n",
      "Epoch 555 : loss [0.09534824]\n",
      "Epoch 556 : loss [0.09465711]\n",
      "Epoch 557 : loss [0.09397294]\n",
      "Epoch 558 : loss [0.09329782]\n",
      "Epoch 559 : loss [0.09263259]\n",
      "Epoch 560 : loss [0.0919775]\n",
      "Epoch 561 : loss [0.09133258]\n",
      "Epoch 562 : loss [0.09069774]\n",
      "Epoch 563 : loss [0.09007284]\n",
      "Epoch 564 : loss [0.08945771]\n",
      "Epoch 565 : loss [0.08885217]\n",
      "Epoch 566 : loss [0.08825603]\n",
      "Epoch 567 : loss [0.08766908]\n",
      "Epoch 568 : loss [0.08709112]\n",
      "Epoch 569 : loss [0.08652194]\n",
      "Epoch 570 : loss [0.08596133]\n",
      "Epoch 571 : loss [0.08540909]\n",
      "Epoch 572 : loss [0.08486501]\n",
      "Epoch 573 : loss [0.08432888]\n",
      "Epoch 574 : loss [0.08380052]\n",
      "Epoch 575 : loss [0.08327972]\n",
      "Epoch 576 : loss [0.0827663]\n",
      "Epoch 577 : loss [0.08226008]\n",
      "Epoch 578 : loss [0.08176088]\n",
      "Epoch 579 : loss [0.08126853]\n",
      "Epoch 580 : loss [0.08078287]\n",
      "Epoch 581 : loss [0.08030373]\n",
      "Epoch 582 : loss [0.07983097]\n",
      "Epoch 583 : loss [0.07936442]\n",
      "Epoch 584 : loss [0.07890396]\n",
      "Epoch 585 : loss [0.07844944]\n",
      "Epoch 586 : loss [0.07800073]\n",
      "Epoch 587 : loss [0.07755769]\n",
      "Epoch 588 : loss [0.0771202]\n",
      "Epoch 589 : loss [0.07668815]\n",
      "Epoch 590 : loss [0.07626141]\n",
      "Epoch 591 : loss [0.07583987]\n",
      "Epoch 592 : loss [0.07542342]\n",
      "Epoch 593 : loss [0.07501196]\n",
      "Epoch 594 : loss [0.07460538]\n",
      "Epoch 595 : loss [0.07420359]\n",
      "Epoch 596 : loss [0.07380648]\n",
      "Epoch 597 : loss [0.07341397]\n",
      "Epoch 598 : loss [0.07302596]\n",
      "Epoch 599 : loss [0.07264237]\n",
      "Epoch 600 : loss [0.07581881]\n",
      "Epoch 601 : loss [0.07697327]\n",
      "Epoch 602 : loss [0.07656134]\n",
      "Epoch 603 : loss [0.07610132]\n",
      "Epoch 604 : loss [0.07559973]\n",
      "Epoch 605 : loss [0.07508845]\n",
      "Epoch 606 : loss [0.07457843]\n",
      "Epoch 607 : loss [0.07407348]\n",
      "Epoch 608 : loss [0.07357501]\n",
      "Epoch 609 : loss [0.07308355]\n",
      "Epoch 610 : loss [0.07259924]\n",
      "Epoch 611 : loss [0.07212204]\n",
      "Epoch 612 : loss [0.07165188]\n",
      "Epoch 613 : loss [0.07118865]\n",
      "Epoch 614 : loss [0.07073222]\n",
      "Epoch 615 : loss [0.07028247]\n",
      "Epoch 616 : loss [0.06983926]\n",
      "Epoch 617 : loss [0.06940246]\n",
      "Epoch 618 : loss [0.06897193]\n",
      "Epoch 619 : loss [0.06854755]\n",
      "Epoch 620 : loss [0.06812917]\n",
      "Epoch 621 : loss [0.06771665]\n",
      "Epoch 622 : loss [0.06730988]\n",
      "Epoch 623 : loss [0.0669087]\n",
      "Epoch 624 : loss [0.066513]\n",
      "Epoch 625 : loss [0.06612264]\n",
      "Epoch 626 : loss [0.0657375]\n",
      "Epoch 627 : loss [0.06535747]\n",
      "Epoch 628 : loss [0.06498242]\n",
      "Epoch 629 : loss [0.06461224]\n",
      "Epoch 630 : loss [0.06424682]\n",
      "Epoch 631 : loss [0.06388606]\n",
      "Epoch 632 : loss [0.06352985]\n",
      "Epoch 633 : loss [0.06317808]\n",
      "Epoch 634 : loss [0.06283067]\n",
      "Epoch 635 : loss [0.06248752]\n",
      "Epoch 636 : loss [0.06214853]\n",
      "Epoch 637 : loss [0.06181363]\n",
      "Epoch 638 : loss [0.06148272]\n",
      "Epoch 639 : loss [0.06115572]\n",
      "Epoch 640 : loss [0.06083255]\n",
      "Epoch 641 : loss [0.06051313]\n",
      "Epoch 642 : loss [0.0601974]\n",
      "Epoch 643 : loss [0.05988527]\n",
      "Epoch 644 : loss [0.05957668]\n",
      "Epoch 645 : loss [0.05927155]\n",
      "Epoch 646 : loss [0.05896983]\n",
      "Epoch 647 : loss [0.05867144]\n",
      "Epoch 648 : loss [0.05837632]\n",
      "Epoch 649 : loss [0.05808442]\n",
      "Epoch 650 : loss [0.06053234]\n",
      "Epoch 651 : loss [0.06138518]\n",
      "Epoch 652 : loss [0.06107321]\n",
      "Epoch 653 : loss [0.06072427]\n",
      "Epoch 654 : loss [0.06034453]\n",
      "Epoch 655 : loss [0.0599575]\n",
      "Epoch 656 : loss [0.05957138]\n",
      "Epoch 657 : loss [0.05918901]\n",
      "Epoch 658 : loss [0.05881141]\n",
      "Epoch 659 : loss [0.05843895]\n",
      "Epoch 660 : loss [0.05807169]\n",
      "Epoch 661 : loss [0.0577096]\n",
      "Epoch 662 : loss [0.05735262]\n",
      "Epoch 663 : loss [0.05700064]\n",
      "Epoch 664 : loss [0.05665359]\n",
      "Epoch 665 : loss [0.05631136]\n",
      "Epoch 666 : loss [0.05597386]\n",
      "Epoch 667 : loss [0.05564101]\n",
      "Epoch 668 : loss [0.0553127]\n",
      "Epoch 669 : loss [0.05498884]\n",
      "Epoch 670 : loss [0.05466934]\n",
      "Epoch 671 : loss [0.05435411]\n",
      "Epoch 672 : loss [0.05404305]\n",
      "Epoch 673 : loss [0.05373607]\n",
      "Epoch 674 : loss [0.05343309]\n",
      "Epoch 675 : loss [0.05313402]\n",
      "Epoch 676 : loss [0.05283876]\n",
      "Epoch 677 : loss [0.05254724]\n",
      "Epoch 678 : loss [0.05225938]\n",
      "Epoch 679 : loss [0.05197509]\n",
      "Epoch 680 : loss [0.0516943]\n",
      "Epoch 681 : loss [0.05141693]\n",
      "Epoch 682 : loss [0.05114292]\n",
      "Epoch 683 : loss [0.05087218]\n",
      "Epoch 684 : loss [0.05060466]\n",
      "Epoch 685 : loss [0.05034028]\n",
      "Epoch 686 : loss [0.05007899]\n",
      "Epoch 687 : loss [0.04982071]\n",
      "Epoch 688 : loss [0.0495654]\n",
      "Epoch 689 : loss [0.04931298]\n",
      "Epoch 690 : loss [0.0490634]\n",
      "Epoch 691 : loss [0.04881662]\n",
      "Epoch 692 : loss [0.04857256]\n",
      "Epoch 693 : loss [0.04833119]\n",
      "Epoch 694 : loss [0.04809245]\n",
      "Epoch 695 : loss [0.04785629]\n",
      "Epoch 696 : loss [0.04762267]\n",
      "Epoch 697 : loss [0.04739153]\n",
      "Epoch 698 : loss [0.04716284]\n",
      "Epoch 699 : loss [0.04693655]\n",
      "Epoch 700 : loss [0.04885969]\n",
      "Epoch 701 : loss [0.04950964]\n",
      "Epoch 702 : loss [0.04926876]\n",
      "Epoch 703 : loss [0.04899893]\n",
      "Epoch 704 : loss [0.04870562]\n",
      "Epoch 705 : loss [0.04840662]\n",
      "Epoch 706 : loss [0.04810828]\n",
      "Epoch 707 : loss [0.04781275]\n",
      "Epoch 708 : loss [0.04752082]\n",
      "Epoch 709 : loss [0.04723275]\n",
      "Epoch 710 : loss [0.04694859]\n",
      "Epoch 711 : loss [0.04666829]\n",
      "Epoch 712 : loss [0.0463918]\n",
      "Epoch 713 : loss [0.04611904]\n",
      "Epoch 714 : loss [0.04584995]\n",
      "Epoch 715 : loss [0.04558445]\n",
      "Epoch 716 : loss [0.04532248]\n",
      "Epoch 717 : loss [0.04506398]\n",
      "Epoch 718 : loss [0.04480886]\n",
      "Epoch 719 : loss [0.04455706]\n",
      "Epoch 720 : loss [0.04430852]\n",
      "Epoch 721 : loss [0.04406318]\n",
      "Epoch 722 : loss [0.04382095]\n",
      "Epoch 723 : loss [0.04358179]\n",
      "Epoch 724 : loss [0.04334562]\n",
      "Epoch 725 : loss [0.04311239]\n",
      "Epoch 726 : loss [0.04288202]\n",
      "Epoch 727 : loss [0.04265447]\n",
      "Epoch 728 : loss [0.04242966]\n",
      "Epoch 729 : loss [0.04220755]\n",
      "Epoch 730 : loss [0.04198808]\n",
      "Epoch 731 : loss [0.04177119]\n",
      "Epoch 732 : loss [0.04155683]\n",
      "Epoch 733 : loss [0.04134495]\n",
      "Epoch 734 : loss [0.04113551]\n",
      "Epoch 735 : loss [0.04092844]\n",
      "Epoch 736 : loss [0.0407237]\n",
      "Epoch 737 : loss [0.04052126]\n",
      "Epoch 738 : loss [0.04032106]\n",
      "Epoch 739 : loss [0.04012305]\n",
      "Epoch 740 : loss [0.03992721]\n",
      "Epoch 741 : loss [0.03973349]\n",
      "Epoch 742 : loss [0.03954184]\n",
      "Epoch 743 : loss [0.03935224]\n",
      "Epoch 744 : loss [0.03916464]\n",
      "Epoch 745 : loss [0.038979]\n",
      "Epoch 746 : loss [0.0387953]\n",
      "Epoch 747 : loss [0.03861349]\n",
      "Epoch 748 : loss [0.03843355]\n",
      "Epoch 749 : loss [0.03825544]\n",
      "Epoch 750 : loss [0.03978983]\n",
      "Epoch 751 : loss [0.0402975]\n",
      "Epoch 752 : loss [0.04010855]\n",
      "Epoch 753 : loss [0.03989658]\n",
      "Epoch 754 : loss [0.03966628]\n",
      "Epoch 755 : loss [0.03943146]\n",
      "Epoch 756 : loss [0.03919709]\n",
      "Epoch 757 : loss [0.03896489]\n",
      "Epoch 758 : loss [0.03873545]\n",
      "Epoch 759 : loss [0.03850898]\n",
      "Epoch 760 : loss [0.0382855]\n",
      "Epoch 761 : loss [0.03806498]\n",
      "Epoch 762 : loss [0.03784737]\n",
      "Epoch 763 : loss [0.03763261]\n",
      "Epoch 764 : loss [0.03742065]\n",
      "Epoch 765 : loss [0.03721143]\n",
      "Epoch 766 : loss [0.03700491]\n",
      "Epoch 767 : loss [0.03680102]\n",
      "Epoch 768 : loss [0.03659972]\n",
      "Epoch 769 : loss [0.03640097]\n",
      "Epoch 770 : loss [0.0362047]\n",
      "Epoch 771 : loss [0.03601088]\n",
      "Epoch 772 : loss [0.03581945]\n",
      "Epoch 773 : loss [0.03563036]\n",
      "Epoch 774 : loss [0.03544357]\n",
      "Epoch 775 : loss [0.03525903]\n",
      "Epoch 776 : loss [0.0350767]\n",
      "Epoch 777 : loss [0.03489652]\n",
      "Epoch 778 : loss [0.03471846]\n",
      "Epoch 779 : loss [0.03454247]\n",
      "Epoch 780 : loss [0.03436852]\n",
      "Epoch 781 : loss [0.03419655]\n",
      "Epoch 782 : loss [0.03402653]\n",
      "Epoch 783 : loss [0.03385843]\n",
      "Epoch 784 : loss [0.0336922]\n",
      "Epoch 785 : loss [0.03352781]\n",
      "Epoch 786 : loss [0.03336523]\n",
      "Epoch 787 : loss [0.03320441]\n",
      "Epoch 788 : loss [0.03304532]\n",
      "Epoch 789 : loss [0.03288795]\n",
      "Epoch 790 : loss [0.03273224]\n",
      "Epoch 791 : loss [0.03257817]\n",
      "Epoch 792 : loss [0.03242571]\n",
      "Epoch 793 : loss [0.03227484]\n",
      "Epoch 794 : loss [0.03212552]\n",
      "Epoch 795 : loss [0.03197772]\n",
      "Epoch 796 : loss [0.03183143]\n",
      "Epoch 797 : loss [0.0316866]\n",
      "Epoch 798 : loss [0.03154323]\n",
      "Epoch 799 : loss [0.03140128]\n",
      "Epoch 800 : loss [0.03264095]\n",
      "Epoch 801 : loss [0.03304534]\n",
      "Epoch 802 : loss [0.03289519]\n",
      "Epoch 803 : loss [0.03272649]\n",
      "Epoch 804 : loss [0.0325432]\n",
      "Epoch 805 : loss [0.03235627]\n",
      "Epoch 806 : loss [0.03216967]\n",
      "Epoch 807 : loss [0.03198475]\n",
      "Epoch 808 : loss [0.03180199]\n",
      "Epoch 809 : loss [0.03162155]\n",
      "Epoch 810 : loss [0.03144344]\n",
      "Epoch 811 : loss [0.03126765]\n",
      "Epoch 812 : loss [0.03109412]\n",
      "Epoch 813 : loss [0.03092281]\n",
      "Epoch 814 : loss [0.03075368]\n",
      "Epoch 815 : loss [0.03058668]\n",
      "Epoch 816 : loss [0.03042178]\n",
      "Epoch 817 : loss [0.03025893]\n",
      "Epoch 818 : loss [0.0300981]\n",
      "Epoch 819 : loss [0.02993924]\n",
      "Epoch 820 : loss [0.02978233]\n",
      "Epoch 821 : loss [0.02962732]\n",
      "Epoch 822 : loss [0.02947418]\n",
      "Epoch 823 : loss [0.02932287]\n",
      "Epoch 824 : loss [0.02917335]\n",
      "Epoch 825 : loss [0.02902559]\n",
      "Epoch 826 : loss [0.02887955]\n",
      "Epoch 827 : loss [0.0287352]\n",
      "Epoch 828 : loss [0.02859251]\n",
      "Epoch 829 : loss [0.02845143]\n",
      "Epoch 830 : loss [0.02831195]\n",
      "Epoch 831 : loss [0.02817403]\n",
      "Epoch 832 : loss [0.02803764]\n",
      "Epoch 833 : loss [0.02790275]\n",
      "Epoch 834 : loss [0.02776933]\n",
      "Epoch 835 : loss [0.02763736]\n",
      "Epoch 836 : loss [0.0275068]\n",
      "Epoch 837 : loss [0.02737762]\n",
      "Epoch 838 : loss [0.02724982]\n",
      "Epoch 839 : loss [0.02712335]\n",
      "Epoch 840 : loss [0.0269982]\n",
      "Epoch 841 : loss [0.02687433]\n",
      "Epoch 842 : loss [0.02675174]\n",
      "Epoch 843 : loss [0.02663039]\n",
      "Epoch 844 : loss [0.02651027]\n",
      "Epoch 845 : loss [0.02639134]\n",
      "Epoch 846 : loss [0.0262736]\n",
      "Epoch 847 : loss [0.02615702]\n",
      "Epoch 848 : loss [0.02604159]\n",
      "Epoch 849 : loss [0.02592727]\n",
      "Epoch 850 : loss [0.02693916]\n",
      "Epoch 851 : loss [0.02726641]\n",
      "Epoch 852 : loss [0.02714579]\n",
      "Epoch 853 : loss [0.02701007]\n",
      "Epoch 854 : loss [0.02686257]\n",
      "Epoch 855 : loss [0.02671209]\n",
      "Epoch 856 : loss [0.02656184]\n",
      "Epoch 857 : loss [0.02641293]\n",
      "Epoch 858 : loss [0.02626573]\n",
      "Epoch 859 : loss [0.02612038]\n",
      "Epoch 860 : loss [0.02597687]\n",
      "Epoch 861 : loss [0.0258352]\n",
      "Epoch 862 : loss [0.02569531]\n",
      "Epoch 863 : loss [0.02555719]\n",
      "Epoch 864 : loss [0.02542078]\n",
      "Epoch 865 : loss [0.02528606]\n",
      "Epoch 866 : loss [0.025153]\n",
      "Epoch 867 : loss [0.02502156]\n",
      "Epoch 868 : loss [0.02489171]\n",
      "Epoch 869 : loss [0.02476344]\n",
      "Epoch 870 : loss [0.02463669]\n",
      "Epoch 871 : loss [0.02451146]\n",
      "Epoch 872 : loss [0.0243877]\n",
      "Epoch 873 : loss [0.02426539]\n",
      "Epoch 874 : loss [0.02414451]\n",
      "Epoch 875 : loss [0.02402502]\n",
      "Epoch 876 : loss [0.0239069]\n",
      "Epoch 877 : loss [0.02379011]\n",
      "Epoch 878 : loss [0.02367464]\n",
      "Epoch 879 : loss [0.02356046]\n",
      "Epoch 880 : loss [0.02344755]\n",
      "Epoch 881 : loss [0.02333587]\n",
      "Epoch 882 : loss [0.02322541]\n",
      "Epoch 883 : loss [0.02311615]\n",
      "Epoch 884 : loss [0.02300805]\n",
      "Epoch 885 : loss [0.0229011]\n",
      "Epoch 886 : loss [0.02279528]\n",
      "Epoch 887 : loss [0.02269057]\n",
      "Epoch 888 : loss [0.02258694]\n",
      "Epoch 889 : loss [0.02248438]\n",
      "Epoch 890 : loss [0.02238287]\n",
      "Epoch 891 : loss [0.02228238]\n",
      "Epoch 892 : loss [0.02218291]\n",
      "Epoch 893 : loss [0.02208443]\n",
      "Epoch 894 : loss [0.02198693]\n",
      "Epoch 895 : loss [0.02189039]\n",
      "Epoch 896 : loss [0.02179479]\n",
      "Epoch 897 : loss [0.02170012]\n",
      "Epoch 898 : loss [0.02160636]\n",
      "Epoch 899 : loss [0.02151349]\n",
      "Epoch 900 : loss [0.02234648]\n",
      "Epoch 901 : loss [0.02261471]\n",
      "Epoch 902 : loss [0.02251694]\n",
      "Epoch 903 : loss [0.02240675]\n",
      "Epoch 904 : loss [0.02228693]\n",
      "Epoch 905 : loss [0.02216466]\n",
      "Epoch 906 : loss [0.02204256]\n",
      "Epoch 907 : loss [0.02192153]\n",
      "Epoch 908 : loss [0.02180188]\n",
      "Epoch 909 : loss [0.02168371]\n",
      "Epoch 910 : loss [0.02156702]\n",
      "Epoch 911 : loss [0.0214518]\n",
      "Epoch 912 : loss [0.02133802]\n",
      "Epoch 913 : loss [0.02122564]\n",
      "Epoch 914 : loss [0.02111464]\n",
      "Epoch 915 : loss [0.021005]\n",
      "Epoch 916 : loss [0.02089668]\n",
      "Epoch 917 : loss [0.02078966]\n",
      "Epoch 918 : loss [0.02068392]\n",
      "Epoch 919 : loss [0.02057944]\n",
      "Epoch 920 : loss [0.02047618]\n",
      "Epoch 921 : loss [0.02037414]\n",
      "Epoch 922 : loss [0.02027328]\n",
      "Epoch 923 : loss [0.02017359]\n",
      "Epoch 924 : loss [0.02007503]\n",
      "Epoch 925 : loss [0.0199776]\n",
      "Epoch 926 : loss [0.01988126]\n",
      "Epoch 927 : loss [0.01978601]\n",
      "Epoch 928 : loss [0.0196918]\n",
      "Epoch 929 : loss [0.01959864]\n",
      "Epoch 930 : loss [0.01950649]\n",
      "Epoch 931 : loss [0.01941533]\n",
      "Epoch 932 : loss [0.01932516]\n",
      "Epoch 933 : loss [0.01923594]\n",
      "Epoch 934 : loss [0.01914767]\n",
      "Epoch 935 : loss [0.01906032]\n",
      "Epoch 936 : loss [0.01897388]\n",
      "Epoch 937 : loss [0.01888833]\n",
      "Epoch 938 : loss [0.01880366]\n",
      "Epoch 939 : loss [0.01871985]\n",
      "Epoch 940 : loss [0.01863688]\n",
      "Epoch 941 : loss [0.01855474]\n",
      "Epoch 942 : loss [0.01847341]\n",
      "Epoch 943 : loss [0.01839289]\n",
      "Epoch 944 : loss [0.01831315]\n",
      "Epoch 945 : loss [0.01823419]\n",
      "Epoch 946 : loss [0.01815599]\n",
      "Epoch 947 : loss [0.01807854]\n",
      "Epoch 948 : loss [0.01800182]\n",
      "Epoch 949 : loss [0.01792583]\n",
      "Epoch 950 : loss [0.01861637]\n",
      "Epoch 951 : loss [0.01883853]\n",
      "Epoch 952 : loss [0.01875868]\n",
      "Epoch 953 : loss [0.01866854]\n",
      "Epoch 954 : loss [0.01857044]\n",
      "Epoch 955 : loss [0.01847031]\n",
      "Epoch 956 : loss [0.01837031]\n",
      "Epoch 957 : loss [0.01827117]\n",
      "Epoch 958 : loss [0.01817316]\n",
      "Epoch 959 : loss [0.01807634]\n",
      "Epoch 960 : loss [0.01798073]\n",
      "Epoch 961 : loss [0.01788631]\n",
      "Epoch 962 : loss [0.01779306]\n",
      "Epoch 963 : loss [0.01770094]\n",
      "Epoch 964 : loss [0.01760994]\n",
      "Epoch 965 : loss [0.01752004]\n",
      "Epoch 966 : loss [0.01743121]\n",
      "Epoch 967 : loss [0.01734343]\n",
      "Epoch 968 : loss [0.01725669]\n",
      "Epoch 969 : loss [0.01717097]\n",
      "Epoch 970 : loss [0.01708624]\n",
      "Epoch 971 : loss [0.0170025]\n",
      "Epoch 972 : loss [0.01691971]\n",
      "Epoch 973 : loss [0.01683787]\n",
      "Epoch 974 : loss [0.01675696]\n",
      "Epoch 975 : loss [0.01667695]\n",
      "Epoch 976 : loss [0.01659784]\n",
      "Epoch 977 : loss [0.0165196]\n",
      "Epoch 978 : loss [0.01644221]\n",
      "Epoch 979 : loss [0.01636567]\n",
      "Epoch 980 : loss [0.01628995]\n",
      "Epoch 981 : loss [0.01621505]\n",
      "Epoch 982 : loss [0.01614094]\n",
      "Epoch 983 : loss [0.0160676]\n",
      "Epoch 984 : loss [0.01599504]\n",
      "Epoch 985 : loss [0.01592322]\n",
      "Epoch 986 : loss [0.01585215]\n",
      "Epoch 987 : loss [0.0157818]\n",
      "Epoch 988 : loss [0.01571216]\n",
      "Epoch 989 : loss [0.01564322]\n",
      "Epoch 990 : loss [0.01557496]\n",
      "Epoch 991 : loss [0.01550738]\n",
      "Epoch 992 : loss [0.01544047]\n",
      "Epoch 993 : loss [0.0153742]\n",
      "Epoch 994 : loss [0.01530858]\n",
      "Epoch 995 : loss [0.01524359]\n",
      "Epoch 996 : loss [0.01517922]\n",
      "Epoch 997 : loss [0.01511546]\n",
      "Epoch 998 : loss [0.01505229]\n",
      "Epoch 999 : loss [0.01498972]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming input_size, x, y, weights_ih, weights_ho, bias_ih, bias_ho, and sigmoid function are defined somewhere\n",
    "\n",
    "LEARNING_RATE = 1\n",
    "NUM_EPOCHS = 1000\n",
    "BATCH_SIZE = 4  # Set your desired batch size\n",
    "cost = []\n",
    "initial_lr = LEARNING_RATE\n",
    "lr_schedule = lambda epoch: initial_lr * 1.15 ** (epoch // 50)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    tot_loss = 0\n",
    "    \n",
    "    current_lr = lr_schedule(epoch)\n",
    "\n",
    "    for batch_start in range(0, len(x_train), BATCH_SIZE):\n",
    "        batch_end = batch_start + BATCH_SIZE\n",
    "        x_batch = x_train[batch_start:batch_end]\n",
    "        y_batch = y_train[batch_start:batch_end]\n",
    "\n",
    "        batch_gradients_ih = np.zeros_like(weights_ih)\n",
    "        batch_gradients_ho = np.zeros_like(weights_ho)\n",
    "        batch_bias_ih = np.zeros_like(bias_ih)\n",
    "        batch_bias_ho = np.zeros_like(bias_ho)\n",
    "\n",
    "        for i, s in enumerate(x_batch):\n",
    "            inp = np.reshape(np.array([int(char) for char in s]), (1, input_size))\n",
    "\n",
    "            hlayer_logits = np.dot(inp, weights_ih) + bias_ih\n",
    "            hlayer_output = sigmoid(hlayer_logits)\n",
    "\n",
    "            final_logits = np.dot(hlayer_output, weights_ho) + bias_ho\n",
    "            final_output = sigmoid(final_logits)\n",
    "\n",
    "            tot_loss += 0.5 * (abs(final_output[0] - y_batch[i])) ** 2\n",
    "\n",
    "            output_delta = (y_batch[i] - final_output[0]) * final_output * (1 - final_output[0])\n",
    "            batch_bias_ho += output_delta\n",
    "            batch_bias_ih += (output_delta.dot(weights_ho.T) * hlayer_output * (1 - hlayer_output))\n",
    "            batch_gradients_ho += hlayer_output.T.dot(output_delta)\n",
    "            batch_gradients_ih += inp.T.dot(output_delta.dot(weights_ho.T) * hlayer_output * (1 - hlayer_output))\n",
    "\n",
    "        # Update weights and biases after the batch\n",
    "        bias_ho += current_lr * (batch_bias_ho / BATCH_SIZE)\n",
    "        bias_ih += current_lr * (batch_bias_ih / BATCH_SIZE)\n",
    "        weights_ho += current_lr * (batch_gradients_ho / BATCH_SIZE)\n",
    "        weights_ih += current_lr * (batch_gradients_ih / BATCH_SIZE)\n",
    "\n",
    "    cost.append(tot_loss)\n",
    "    print(f'Epoch {epoch} : loss {tot_loss}')\n",
    "    if epoch % 10 == 0:\n",
    "        plt.plot([pl for pl in range(epoch + 1)], cost, 'b')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        # plt.ylim(0, 5)\n",
    "        plt.xlim(0, NUM_EPOCHS)\n",
    "        plt.savefig('train.png')\n",
    "        # if tot_loss < min_loss:       # IMP: Don't save weights\n",
    "        #     np.save('weights_ih2.npy', weights_ih)\n",
    "        #     np.save('weights_ho2.npy', weights_ho)\n",
    "        #     np.save('bias_ih2', bias_ih)\n",
    "        #     np.save(\"bias_ho2\", bias_ho)\n",
    "        #     min_loss = tot_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-15.7104567 ,  15.242776  ],\n",
       "       [-35.90671388,  34.78123423],\n",
       "       [  8.93543973,  -8.69365667],\n",
       "       [-18.03693242,  17.4795214 ],\n",
       "       [-13.39948743,  13.03934688],\n",
       "       [ 13.38699901, -13.0054465 ],\n",
       "       [ 17.96298562, -17.40303231],\n",
       "       [ -8.98574793,   8.72942564],\n",
       "       [ 35.91996659, -34.81761997],\n",
       "       [ 15.74307968, -15.22940285]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_ih"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weights_ih: \n",
      " [[-15.7104567   15.242776  ]\n",
      " [-35.90671388  34.78123423]\n",
      " [  8.93543973  -8.69365667]\n",
      " [-18.03693242  17.4795214 ]\n",
      " [-13.39948743  13.03934688]\n",
      " [ 13.38699901 -13.0054465 ]\n",
      " [ 17.96298562 -17.40303231]\n",
      " [ -8.98574793   8.72942564]\n",
      " [ 35.91996659 -34.81761997]\n",
      " [ 15.74307968 -15.22940285]]\n",
      "\n",
      "bias_ih: \n",
      " [[-1.78542013 -1.86027718]]\n",
      "weights_ho: \n",
      " [[-23.11560567]\n",
      " [-23.43950148]]\n",
      "\n",
      "bias_ho: \n",
      " [[10.22709907]]\n"
     ]
    }
   ],
   "source": [
    "print('\\nweights_ih: \\n', weights_ih)\n",
    "print('\\nbias_ih: \\n', bias_ih)\n",
    "print('weights_ho: \\n', weights_ho)\n",
    "print('\\nbias_ho: \\n', bias_ho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- \n",
      "Input: \n",
      " [[0 0 0 1 1 1 1 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-10.85760329   6.97953793]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.92572577e-05 9.99070132e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-13.19105192]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.86723135e-06]]\n",
      "\n",
      "Not Palindrome!\n"
     ]
    }
   ],
   "source": [
    "x_test1 = '0001111100'\n",
    "x_test2 = '1000011111'\n",
    "def test(x):\n",
    "    inp = np.reshape(np.array([int(char) for char in x]), (1, input_size))\n",
    "    print('-'*25, '\\nInput: \\n', inp)\n",
    "    print('\\nLogit 1: \\n', np.dot(inp, weights_ih)+ bias_ih)\n",
    "    print('\\nHidden layer 1 Output: \\n', sigmoid(np.dot(inp, weights_ih)+ bias_ih))\n",
    "    print('\\nLogit 2: \\n', np.dot(sigmoid(np.dot(inp, weights_ih)+ bias_ih), weights_ho)+bias_ho)\n",
    "    print('\\nHidded layer 2 Output: \\n', sigmoid(np.dot(sigmoid(np.dot(inp, weights_ih)+ bias_ih), weights_ho)+bias_ho))\n",
    "    return sigmoid(np.dot(sigmoid(np.dot(inp, weights_ih)+ bias_ih), weights_ho)+bias_ho)\n",
    "\n",
    "if(test(x_test1)>0.5):\n",
    "    print(\"\\nPalindrome\")\n",
    "else:\n",
    "    print(\"\\nNot Palindrome!\")\n",
    "\n",
    "\n",
    "# if(test(x_test2)>0.5):\n",
    "#     print(\"\\nPalindrome\")\n",
    "# else:\n",
    "#     print(\"\\nNot Palindrome!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-15.64619003  15.04597634]\n",
      " [-35.75595564  34.55811084]\n",
      " [  8.89220477  -8.67068415]\n",
      " [-17.84405789  17.27466329]\n",
      " [-13.4049504   12.95376822]\n",
      " [ 13.38969871 -12.94516135]\n",
      " [ 17.91779276 -17.27506641]\n",
      " [ -8.91845884   8.67530989]\n",
      " [ 35.70260643 -34.51291818]\n",
      " [ 15.61102179 -15.08297522]]\n"
     ]
    }
   ],
   "source": [
    "np.save('new_weights/weights_ih2.npy', weights_ih)\n",
    "np.save('new_weights/weights_ho2.npy', weights_ho)\n",
    "np.save('new_weights/bias_ih2', bias_ih)\n",
    "np.save(\"new_weights/bias_ho2\", bias_ho)\n",
    "# weights_ih = np.load('weights_ih2.npy')\n",
    "# weights_ho = np.load('weights_ho2.npy')\n",
    "# bias_ih = np.load('bias_ih2.npy')\n",
    "# bias_ho = np.load('bias_ho2.npy')\n",
    "print(weights_ih)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 1 0 0 1 0 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-12.73080762   6.22583002]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[2.95855206e-06 9.98026222e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.79690906]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.37876263e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 0 1 0 0 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-17.95843979  17.1512785 ]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.58762778e-08 9.99999964e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753314]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387412e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 0 1 1 0 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-14.13341558   7.83920717]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[7.27671372e-07 9.99606174e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.82943063]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.3346444e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 0 0 1 1 1 0 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 22.40509152 -30.51419525]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 5.59568661e-14]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.9672667]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16279761e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 0 0 1 0 1 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 56.18508145 -61.67619096]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 1.63821387e-27]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 1 1 1 1 1 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-17.5002438   13.19608518]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[2.51038699e-08 9.99998142e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83749577]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32392359e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 0 0 0 1 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 68.30484582 -64.67751787]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 8.14537133e-29]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 1 0 0 0 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-89.39854218  86.85964266]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.49522462e-39 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 0 0 0 1 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-12.39671918  11.73378763]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[4.13210618e-06 9.99991982e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83745398]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32397892e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 0 0 0 0 1 0 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 102.03863194 -100.46139002]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 2.34516105e-44]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 1 1 1 1 0 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 48.07542224 -49.75755214]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 2.45793091e-22]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 0 1 0 0 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-29.60285542  29.36984439]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.39201285e-13 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 1 0 0 1 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 1.34976735 -0.79416193]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[0.79409159 0.31127572]]\n",
      "\n",
      "Logit 2: \n",
      " [[-18.1125097]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.36093373e-08]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 1 0 0 1 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 1.34976735 -0.79416193]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[0.79409159 0.31127572]]\n",
      "\n",
      "Logit 2: \n",
      " [[-18.1125097]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.36093373e-08]]\n",
      "1111001111  - predicted:  0  with prob:  [[1.36093373e-08]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 1 0 1 0 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-46.55536408  44.02345712]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[6.04313472e-21 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 0 0 1 1 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-33.53804605  25.33046493]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[2.72026783e-15 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 1 1 0 0 1 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 75.09595479 -75.44890083]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 1.70985273e-33]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 1 1 0 0 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-82.59451827  74.75336425]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.3478963e-36 1.0000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 0 0 1 1 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 98.50285083 -95.49309268]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 3.37191631e-42]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 0 0 0 1 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 27.61568214 -27.13707911]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 1.63876352e-12]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 0 1 0 0 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-5.31326669  5.13066778]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[0.00490166 0.99412214]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.81804701]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.34992428e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 1 0 0 0 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-30.38091991  21.48984137]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[6.39343313e-14 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753354]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387359e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 0 1 0 0 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-61.1718168   56.49806043]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[2.71279837e-27 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 0 1 1 1 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-42.89765599  32.39789294]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[2.34306706e-19 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 0 1 0 1 0 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-5.59269526 -0.37249079]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[0.00371115 0.4079393 ]]\n",
      "\n",
      "Logit 2: \n",
      " [[-3.71025532]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[0.02388674]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 0 0 0 0 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-29.72283637  25.86532616]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.23462816e-13 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 0 1 0 0 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-97.66575736  88.21583147]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[3.83972841e-43 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 0 1 0 0 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-82.26042983  80.26132186]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.88256182e-36 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 1 0 0 1 1 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-16.25577841   7.83421803]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[8.71375005e-08 9.99604205e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.82937676]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.3347163e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 1 1 0 1 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-8.00984259  6.27326608]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[3.32066699e-04 9.98117492e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.80561709]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.36680845e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 1 1 0 0 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-135.25209268  125.64484171]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.82289848e-59 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 1 1 1 0 1 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-48.69900627  43.81370484]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[7.08419166e-22 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 1 1 0 0 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-123.60767705  113.42627582]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[2.07906437e-54 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 1 0 1 0 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-11.82733053   2.89283244]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[7.30217847e-06 9.47490979e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-14.75530672]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[3.90707314e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 1 1 1 0 0 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-104.05333021   95.02722206]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[6.45970772e-46 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 0 1 0 1 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-5.91669036  0.75998204]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[0.00268686 0.68134983]]\n",
      "\n",
      "Logit 2: \n",
      " [[-9.32487129]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[8.91704845e-05]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 1 0 1 1 0 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-58.47920828  50.73886445]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[4.00684496e-26 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 1 1 1 0 1 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-11.70734958   6.39735067]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[8.23301876e-06 9.98336806e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.80342059]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.36981395e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 0 0 0 1 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-36.6863079   35.97296425]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.16772053e-16 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 1 1 1 0 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 26.98680921 -25.99429072]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 5.13834169e-12]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 1 0 0 0 1 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[  8.69189385 -12.02947379]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99832087e-01 5.96572623e-06]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96390666]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16671122e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 1 1 1 0 0 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 15.11726279 -23.00807844]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99999728e-01 1.01793136e-10]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726107]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16280416e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 0 1 1 1 0 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 80.69980096 -76.68301397]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 4.97725828e-34]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 1 1 1 0 1 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 3.95856814 -7.07777263]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.81267188e-01 8.42939032e-04]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.59606913]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.68543948e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 1 1 1 1 0 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-44.27055839  38.87231924]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[5.93664092e-20 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 0 1 1 1 0 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 12.96089412 -18.96949971]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99997650e-01 5.77631604e-09]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96721807]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16285416e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 1 0 0 1 1 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 66.93388945 -68.36487664]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 2.03944181e-30]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 1 1 0 0 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-108.20234952  105.47176621]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.01933439e-47 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 0 0 1 1 1 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[  7.15111763 -12.20997383]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99216627e-01 4.98050728e-06]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.95111992]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.18172544e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 1 1 0 1 1 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 93.64954417 -94.04590975]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 1.43344274e-41]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 1 1 0 0 0 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-4.00793912 -0.01366212]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[0.01784652 0.49658452]]\n",
      "\n",
      "Logit 2: \n",
      " [[-5.83072482]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[0.00292735]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 0 0 1 1 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 40.88490375 -47.99384598]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 1.43396162e-21]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 1 0 0 1 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 42.36292613 -39.46707351]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 7.23881967e-18]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 1 0 0 0 0 0 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 34.25515857 -36.11224438]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 2.07324935e-16]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 0 0 0 0 1 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 49.70505263 -50.70238537]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.0000000e+00 9.5550687e-23]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 0 1 0 1 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[  3.52750704 -10.7847135 ]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.71460376e-01 2.07133071e-05]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.37569851]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[2.10096535e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 1 0 1 0 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-21.27152793  14.43752797]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[5.77953366e-10 9.99999463e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83752249]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32388821e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 0 1 0 1 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 17.9320771  -18.93717827]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99999984e-01 5.96606486e-09]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726649]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16279785e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 0 1 1 1 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-25.29211023  23.76951298]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.03699364e-11 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 0 0 0 1 1 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 41.25510266 -44.50444237]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 4.69857605e-20]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 1 1 1 0 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 0.93782352 -5.62326005]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[0.71865981 0.00359984]]\n",
      "\n",
      "Logit 2: \n",
      " [[-10.20565355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[3.69594014e-05]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 0 1 0 0 0 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 22.2766545 -29.4016361]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 1.70228269e-13]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.9672667]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16279761e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 0 0 1 1 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 15.6010676  -18.40791683]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99999832e-01 1.01284571e-08]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726344]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1628014e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 0 0 1 1 0 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-17.01039617   7.18264954]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[4.09712101e-08 9.99240924e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.82188743]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.34474996e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 0 1 1 1 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 0.99248339 -5.618461  ]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[0.72957816 0.00361709]]\n",
      "\n",
      "Logit 2: \n",
      " [[-10.4324877]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[2.94588211e-05]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 1 0 1 0 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 19.74163085 -24.2353836 ]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99999997e-01 2.98336668e-11]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726665]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16279767e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 0 1 1 1 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-63.98626902  56.16115437]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.62598468e-28 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 0 0 0 0 1 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 10.01013638 -18.50869915]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99955060e-01 9.15744016e-09]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96633471]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16388183e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 1 0 0 1 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-3.61060532 -4.18639269]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[0.0263238  0.01497341]]\n",
      "\n",
      "Logit 2: \n",
      " [[3.92097183]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[0.98056345]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 1 0 0 0 0 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 28.23778463 -25.81145017]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 6.16921093e-12]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 1 0 1 0 0 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-85.24952287  76.41509851]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.47551083e-38 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 1 0 0 1 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 1.67376245 -1.92663476]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[0.84207681 0.12712353]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.31189963]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[2.23937274e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 1 0 0 1 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 1.67376245 -1.92663476]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[0.84207681 0.12712353]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.31189963]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[2.23937274e-07]]\n",
      "1011001101  - predicted:  0  with prob:  [[2.23937274e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 0 0 0 0 0 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-13.55920252   6.69161972]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.29214903e-06 9.98760268e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.81200553]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.35810451e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 0 0 1 0 0 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-35.56398555  25.77965846]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[3.5872143e-16 1.0000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 1 0 1 1 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-22.98602518  18.82313825]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.04062942e-10 9.99999993e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753341]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387376e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 1 0 0 1 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-78.03355513  69.13791821]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.28960766e-34 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 0 1 0 0 1 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 49.04696909 -55.07787016]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 1.20222267e-24]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 0 1 0 1 0 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 15.54640773 -18.41271588]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99999823e-01 1.00799665e-08]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726324]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16280163e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 1 1 1 1 0 1 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-31.63189533  21.30700082]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.82996340e-14 9.99999999e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753354]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.3238736e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 0 1 1 0 1 0 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 57.29290492 -56.63882211]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 2.52389973e-25]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 0 1 1 0 1 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 14.23457011 -18.81309368]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99999342e-01 6.75425143e-09]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.9672532]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1628133e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 0 0 1 0 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[  4.67039176 -10.77291638]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.90718357e-01 2.09591062e-05]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.77517036]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.40906328e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 1 0 0 1 1 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 26.9575986 -31.5125639]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.000000e+00 2.061899e-14]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 1 0 1 1 0 0 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 34.00494061 -36.09712976]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 2.10482375e-16]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 0 0 0 1 1 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-11.40247175   6.3870351 ]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.11677218e-05 9.98319589e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.80312657]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.37021675e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 0 0 0 0 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[  9.96556985 -11.87306776]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99953012e-01 6.97572189e-06]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96643583]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16376415e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 0 1 1 0 0 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-25.4538361   18.92530024]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[8.82143904e-12 9.99999994e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753342]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387374e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 0 0 1 0 1 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 33.03837745 -37.42521723]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 5.57743698e-17]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 1 0 0 1 0 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-77.03279766  69.33587338]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[3.5081734e-34 1.0000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 0 0 0 0 1 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-26.52837071  19.84470327]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[3.01213883e-12 9.99999998e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.8375335]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387365e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 1 1 1 0 0 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 39.63198231 -38.01490144]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 3.09270199e-17]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 1 0 1 0 1 0 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 94.87924024 -94.06783235]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.0000000e+00 1.4023599e-41]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 0 0 1 1 1 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 36.80966158 -38.66666002]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 1.61169522e-17]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 1 0 0 1 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-10.29464828  11.42440396]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[3.38124292e-05 9.99989075e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83800971]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32324335e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 0 1 0 0 1 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-45.33217805  38.45682682]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[2.05345514e-20 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 1 1 1 1 1 0 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 34.83333551 -36.56291946]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 1.32107002e-16]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 1 0 1 0 1 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-45.56111665  38.67670459]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.63327174e-20 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 0 1 1 1 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-12.64693713  11.74890225]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[3.21738915e-06 9.99992102e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83743749]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32400076e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 1 0 1 1 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-6.82239133 -0.35056818]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[0.00108793 0.41324464]]\n",
      "\n",
      "Logit 2: \n",
      " [[-3.76520215]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[0.02263855]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 0 1 0 0 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-23.24280755  14.89152056]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[8.04964884e-11 9.99999659e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83752652]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32388288e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 1 0 0 0 1 1 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 73.98813132 -80.48626968]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 1.10983191e-35]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 1 0 1 1 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-10.34085209   6.80252752]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[3.22857573e-05 9.98890269e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.81532816]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.35359952e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 1 0 1 1 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 32.87252492 -32.54425441]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 7.34868941e-15]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 0 0 1 0 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 28.51915923 -30.47007669]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 5.84808707e-14]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 0 0 0 0 0 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 10.28956495 -13.00554059]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99966015e-01 2.24783544e-06]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.9666081]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16356368e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 0 0 0 1 0 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 49.38105753 -49.56991255]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 2.96524452e-22]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 0 0 1 0 0 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 20.07571929 -18.72742599]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99999998e-01 7.35838029e-09]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726682]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16279747e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 0 1 1 0 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[  9.71535189 -11.85795314]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99939654e-01 7.08195738e-06]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96616093]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16408411e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 0 1 1 1 0 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 1.31647849 -6.75093382]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[0.78859522 0.00116842]]\n",
      "\n",
      "Logit 2: \n",
      " [[-11.60619901]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[9.1093598e-06]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 1 0 1 0 0 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-11.59839193   2.67295467]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.18075505e-06 9.35411772e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-14.50635473]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[5.01152578e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 0 1 0 1 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-17.56110599  12.97854793]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[2.36215592e-08 9.99997691e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83748644]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32393595e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 1 0 0 0 0 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 39.88220026 -38.03001606]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 3.04630846e-17]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 0 0 0 0 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-90.50636565  81.82227381]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[4.93838358e-40 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 1 1 0 0 1 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-17.25002584  13.18097056]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[3.22410331e-08 9.99998114e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83749534]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32392417e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 1 0 1 1 1 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 48.45407721 -50.88522591]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 7.95842586e-23]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 0 0 1 0 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-19.29520186  12.33378741]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[4.17061693e-09 9.99995599e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83744293]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32399356e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 1 1 1 0 0 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-62.82414777  57.46932817]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[5.19779676e-28 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 1 0 0 1 0 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 13.43782679 -23.6480399 ]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99998541e-01 5.36768179e-11]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96723645]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16283279e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 0 1 0 1 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 49.50103849 -46.06539431]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 9.86400861e-21]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 1 0 1 1 0 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 32.54852982 -31.41178159]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.0000000e+00 2.2805342e-14]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 1 0 0 1 0 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 13.99494044 -12.81477266]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99999164e-01 2.72028099e-06]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96730545]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16275256e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 1 0 0 1 1 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 15.31318297 -19.29399801]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99999776e-01 4.17564076e-09]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726215]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1628029e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 1 1 0 1 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-81.43203493  79.79553215]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[4.31039014e-36 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 0 1 1 0 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 13.24032269 -13.46634115]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99998223e-01 1.41788736e-06]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96725907]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16280649e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 1 0 0 1 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-41.53961456  37.42014718]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.11117974e-19 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 1 0 1 0 0 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-32.59194846  25.52362104]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[7.00642745e-15 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 0 0 1 0 0 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 4.13093071 -6.41402776]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[0.98418618 0.00163573]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.6729595]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.56070243e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 1 1 0 0 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-96.55793389  93.25320032]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.16257807e-42 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 0 1 1 1 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 65.29447343 -68.72850436]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 1.41771785e-30]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 1 0 1 1 1 0 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 60.77525521 -61.77336382]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 1.48651387e-27]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 1 0 1 1 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 58.15636107 -62.13018355]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 1.04040905e-27]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 1 1 0 1 1 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-3.49062437 -0.68187446]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[0.02958018 0.33584307]]\n",
      "\n",
      "Logit 2: \n",
      " [[-2.76072144]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[0.05948399]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 1 0 1 0 1 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[  7.09645776 -12.21477288]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99172652e-01 4.95666296e-06]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.95020727]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.18280444e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 1 0 0 0 1 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-3.95327924 -0.00886307]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[0.01883028 0.49778425]]\n",
      "\n",
      "Logit 2: \n",
      " [[-5.87586103]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[0.00279852]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 1 0 1 1 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 19.90335673 -19.39117086]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99999998e-01 3.78897287e-09]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726674]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16279757e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 0 0 0 1 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 0.56593898 -6.96400361]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[0.63782559 0.00094441]]\n",
      "\n",
      "Logit 2: \n",
      " [[-8.47417892]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[0.00020875]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 1 1 0 1 0 1 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 39.82948641 -48.19660019]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.0000000e+00 1.1707994e-21]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 1 0 0 0 0 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-24.41978978  25.08002729]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[2.48096164e-11 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 0 1 1 1 1 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 10.86774189 -13.45621567]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99980937e-01 1.43231706e-06]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96690081]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16322314e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 0 0 1 0 1 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 44.68279308 -49.64378312]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 2.75409508e-22]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 1 1 0 1 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-28.77446052  28.90405469]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[3.18720926e-13 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 0 0 1 0 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-32.25786002  31.03157865]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.78564363e-15 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 1 1 0 1 1 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 26.58739969 -35.00196751]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 6.29272356e-16]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 1 1 1 1 1 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 22.18816242 -24.54230874]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 2.19487981e-11]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.9672667]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16279761e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 0 0 1 0 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 19.07496183 -18.92538116]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99999995e-01 6.03686402e-09]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726672]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16279758e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 1 0 1 1 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-31.42946512  30.56578895]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[2.24056078e-14 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 0 1 1 1 0 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 49.13083958 -49.55479793]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 3.01040349e-22]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 0 1 1 0 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 52.9287289  -51.20473507]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 5.78184055e-23]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 0 0 1 0 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 7.7545413  -7.83928809]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99571393e-01 3.93794263e-04]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.9664935]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16369703e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 0 1 0 0 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-8.83823749  6.73905579]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.45057179e-04 9.98817635e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.81617015]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.35246028e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 1 0 0 0 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-76.75336909  74.83903194]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[4.63911712e-34 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 1 0 0 0 0 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-57.773337    45.84044649]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[8.1163084e-26 1.0000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 1 0 1 1 0 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 73.5616886  -70.08469316]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 3.65262001e-31]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 1 0 0 1 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-28.89444147  25.39953645]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[2.82685487e-13 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 1 0 0 0 0 1 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 47.21781673 -54.81003562]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 1.57146234e-24]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 1 1 1 0 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-37.31518083  37.11575264]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[6.22619725e-17 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 1 0 0 0 1 0 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 113.68304757 -112.67995591]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 1.15802232e-49]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 0 1 0 1 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-18.23786836  11.64811994]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.20058913e-08 9.99991265e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83735373]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32411165e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 1 0 0 1 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-38.33863887  36.94423199]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[2.23738486e-17 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 0 1 1 0 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 1.59590706 -1.24777526]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[0.83144556 0.22308549]]\n",
      "\n",
      "Logit 2: \n",
      " [[-17.0694584]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[3.86214336e-08]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 0 1 1 0 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 1.59590706 -1.24777526]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[0.83144556 0.22308549]]\n",
      "\n",
      "Logit 2: \n",
      " [[-17.0694584]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[3.86214336e-08]]\n",
      "1000110001  - predicted:  0  with prob:  [[3.86214336e-08]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 0 1 0 0 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-109.31017299  100.43439736]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[3.36662741e-48 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 0 0 0 1 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-63.73605106  56.14603975]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[2.08826075e-28 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 0 1 1 1 0 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-3.25739961  0.19940767]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[0.0370619  0.54968738]]\n",
      "\n",
      "Logit 2: \n",
      " [[-7.32392714]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[0.00065913]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 1 1 1 1 0 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 2.10030686 -0.58109215]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[0.890933   0.35868133]]\n",
      "\n",
      "Logit 2: \n",
      " [[-21.09846411]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[6.87153021e-10]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 1 1 1 1 0 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 2.10030686 -0.58109215]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[0.890933   0.35868133]]\n",
      "\n",
      "Logit 2: \n",
      " [[-21.09846411]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[6.87153021e-10]]\n",
      "1101111011  - predicted:  0  with prob:  [[6.87153021e-10]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 0 1 0 1 1 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 67.60055847 -73.67487909]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 1.00787473e-32]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 1 0 1 0 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-33.59270593  25.32566588]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[2.57556897e-15 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 0 0 0 0 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-50.8114494   49.62858758]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[8.56777216e-23 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 1 0 1 1 1 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 33.72551205 -41.60028832]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.0000000e+00 8.5748067e-19]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 1 0 0 1 0 0 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 26.08299989 -35.66865063]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 3.23073885e-16]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 1 1 0 1 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-68.78686183  67.77492143]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.33735122e-30 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 1 1 1 1 0 0 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[  8.06302092 -10.8866854 ]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99685125e-01 1.87052915e-05]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96112086]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16996598e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 0 1 1 1 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 40.01063728 -39.14257521]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 1.00137231e-17]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 0 1 0 0 0 1 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[  9.63148141 -17.38102537]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99934375e-01 2.82824207e-08]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96590603]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16438087e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 1 1 0 1 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-96.83736246  87.75004176]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[8.7915984e-43 1.0000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 1 1 0 0 0 1 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-31.38167738  21.2918862 ]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[2.35023171e-14 9.99999999e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753354]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.3238736e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 0 1 1 0 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-11.37326114  11.90530829]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.14987453e-05 9.99993246e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83763284]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32374214e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 0 1 1 0 1 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-17.33439127   8.31512236]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[2.96325832e-08 9.99755273e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83248956]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.33056805e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 0 1 1 1 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 68.05462787 -64.66240324]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 8.26942066e-29]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 1 1 0 0 0 0 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-18.41250918   8.13880265]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00820503e-08 9.99708099e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83151674]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.33186308e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 0 0 1 0 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-50.86416325  39.46200345]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[8.12782934e-23 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 1 0 0 1 1 0 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 52.85331448 -61.34488469]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 2.28168184e-27]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 0 0 0 1 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-11.07847665   5.25456228]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.54408789e-05 9.94803512e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.73073766]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.47308353e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 1 1 0 1 1 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-35.05958575  26.44634158]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[5.94039565e-16 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 0 0 0 0 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-13.88319762   7.82409255]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.34552011e-07 9.99600179e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.82931134]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.33480362e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 1 1 1 1 0 0 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-71.32030155  59.04539474]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.06166277e-31 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 1 1 0 0 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-58.62892465  51.64666045]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[3.44970173e-26 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 1 0 0 1 0 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-24.69921835  19.57686873]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.87614225e-11 9.99999997e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753348]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387367e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 1 0 1 0 0 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-49.52740117  44.27949454]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[3.09401894e-22 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 1 1 0 0 0 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 21.07839293 -19.41789251]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99999999e-01 3.68906604e-09]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726677]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16279753e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 0 1 1 0 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-63.70684045  61.66431293]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[2.15015978e-28 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 1 0 0 0 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 27.23702717 -26.00940534]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 5.06126158e-12]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 1 0 1 1 0 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 74.01734193 -74.9679965 ]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 2.76574924e-33]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 1 0 1 1 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 20.22735183 -20.52364368]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99999998e-01 1.22094134e-09]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.9672667]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16279761e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 0 0 1 0 1 0 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 83.23482461 -81.84926646]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 2.83997964e-36]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 0 1 1 1 1 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-50.23327245  49.1779125 ]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.52744987e-22 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 0 1 1 1 1 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-10.22087114  10.30704576]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[3.64012465e-05 9.99966604e-01]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83760022]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32378532e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 1 0 0 1 1 1 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 92.5417207  -99.08327861]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 9.30419601e-44]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 1 1 1 0 1 0 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-63.10357634  51.96616961]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[3.93065574e-28 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 1 1 0 0 1 0 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[  8.35780541 -17.53743141]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.99765496e-01 2.41874622e-08]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96240292]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16846698e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 0 0 0 1 0 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 13.21111207 -18.98461434]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.9999817e-01 5.6896657e-09]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96722886]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.16284161e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 1 0 0 0 1 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-25.09655215  23.7495993 ]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.26097329e-11 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 0 1 1 1 0 1 1 1 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 35.20353442 -33.07351585]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 4.32867091e-15]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[1 1 1 1 0 0 0 0 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-92.15869662  82.79354155]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.4620796e-41 1.0000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 0 1 1 0 0 0 1]]\n",
      "\n",
      "Logit 1: \n",
      " [[-29.97305432  25.88044078]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[9.6131983e-14 1.0000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 0 0 0 1 1 1 1 1 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[ 62.09349773 -68.25258917]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.00000000e+00 2.28179776e-30]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.96726671]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.1627976e-07]]\n",
      "------------------------- \n",
      "Input: \n",
      " [[0 1 0 1 1 1 1 1 0 0]]\n",
      "\n",
      "Logit 1: \n",
      " [[-68.83957568  57.6083373 ]]\n",
      "\n",
      "Hidden layer 1 Output: \n",
      " [[1.26868015e-30 1.00000000e+00]]\n",
      "\n",
      "Logit 2: \n",
      " [[-15.83753355]]\n",
      "\n",
      "Hidded layer 2 Output: \n",
      " [[1.32387358e-07]]\n",
      "-------------------------------- Accuracy:  0.9804878048780488 --------------------------------\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in range(len(x_test)):\n",
    "    y_p = 1 if test(x_test[i])>0.5 else 0\n",
    "    if y_test[i]!=y_p:\n",
    "        count+=1\n",
    "        print(x_test[i], ' - predicted: ', y_p, ' with prob: ', test(x_test[i]))\n",
    "print('-------------------------------- Accuracy: ', (len(x_test) - count)/len(x_test), '--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-1 * x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "palindromes = []\n",
    "for i in range(1024):\n",
    "    binary_string = format(i, '010b')\n",
    "    x.append(binary_string)\n",
    "    if binary_string == binary_string[::-1]:\n",
    "        palindromes.append(binary_string)\n",
    "x = np.array(x)\n",
    "palindromes = np.array(palindromes)\n",
    "\n",
    "y = []\n",
    "for binary_string in x:\n",
    "    y.append(binary_string == binary_string[::-1])\n",
    "y = np.array(y)\n",
    "permutation_index = np.random.permutation(len(x))\n",
    "x = x[permutation_index]\n",
    "y = y[permutation_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(28)\n",
    "LEARNING_RATE = 0.1\n",
    "NUM_EPOCHS = 10000\n",
    "\n",
    "input_size = 10\n",
    "hidden_layer_size = 2\n",
    "output_size = 1\n",
    "global weights_ih\n",
    "\n",
    "# weights_ih = np.random.rand(input_size, hidden_layer_size)\n",
    "weights_ih = np.load('weights/weights_ih2.npy')\n",
    "weights_ho = np.random.rand(hidden_layer_size, output_size)\n",
    "# bias_ih = np.random.rand(1, hidden_layer_size)\n",
    "bias_ih = np.load('weights/bias_ih2.npy')\n",
    "bias_ho = np.random.rand(1, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(predicted, target):\n",
    "    if len(target)!=len(predicted):\n",
    "        raise ValueError(\"Both predicted and target vectors should be same size!\")\n",
    "\n",
    "    correct = 0\n",
    "    for i in range(len(target)):\n",
    "        if target[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(target)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train,y_train, x_val, y_val, lr=1, epochs=1000, batch_size=4):\n",
    "    global weights_ih, weights_ho, bias_ih, bias_ho\n",
    "    LEARNING_RATE = lr\n",
    "    NUM_EPOCHS = epochs\n",
    "    BATCH_SIZE = batch_size  # Set your desired batch size\n",
    "    cost = []\n",
    "    cost_val = []\n",
    "    initial_lr = LEARNING_RATE\n",
    "    lr_schedule = lambda epoch: initial_lr * 0.95 ** (epoch // 50)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        tot_loss = 0\n",
    "        tot_loss_valid = 0\n",
    "        current_lr = lr_schedule(epoch)\n",
    "\n",
    "        for batch_start in range(0, len(x_train), BATCH_SIZE):\n",
    "            batch_end = batch_start + BATCH_SIZE\n",
    "            x_batch = x_train[batch_start:batch_end]\n",
    "            y_batch = y_train[batch_start:batch_end]\n",
    "\n",
    "            batch_gradients_ih = np.zeros_like(weights_ih)\n",
    "            batch_gradients_ho = np.zeros_like(weights_ho)\n",
    "            batch_bias_ih = np.zeros_like(bias_ih)\n",
    "            batch_bias_ho = np.zeros_like(bias_ho)\n",
    "\n",
    "            for i, s in enumerate(x_batch):\n",
    "                inp = np.reshape(np.array([int(char) for char in s]), (1, input_size))\n",
    "\n",
    "                hlayer_logits = np.dot(inp, weights_ih) + bias_ih\n",
    "                hlayer_output = sigmoid(hlayer_logits)\n",
    "\n",
    "                final_logits = np.dot(hlayer_output, weights_ho) + bias_ho\n",
    "                final_output = sigmoid(final_logits)\n",
    "\n",
    "                tot_loss += abs(final_output[0] - y_batch[i])\n",
    "\n",
    "                output_delta = (y_batch[i] - final_output[0]) * final_output * (1 - final_output[0])\n",
    "                batch_bias_ho += output_delta\n",
    "                batch_bias_ih += (output_delta.dot(weights_ho.T) * hlayer_output * (1 - hlayer_output))\n",
    "                batch_gradients_ho += hlayer_output.T.dot(output_delta)\n",
    "                batch_gradients_ih += inp.T.dot(output_delta.dot(weights_ho.T) * hlayer_output * (1 - hlayer_output))\n",
    "\n",
    "            # Update weights and biases after the batch\n",
    "            bias_ho += current_lr * (batch_bias_ho / BATCH_SIZE)\n",
    "            bias_ih += current_lr * (batch_bias_ih / BATCH_SIZE)\n",
    "            weights_ho += current_lr * (batch_gradients_ho / BATCH_SIZE)\n",
    "            weights_ih += current_lr * (batch_gradients_ih / BATCH_SIZE)\n",
    "        \n",
    "        cost.append(tot_loss)\n",
    "        # print(f'Epoch {epoch} : loss {tot_loss}')\n",
    "        for i, s in enumerate(x_val):\n",
    "                inp = np.reshape(np.array([int(char) for char in s]), (1, input_size))\n",
    "\n",
    "                hlayer_logits = np.dot(inp, weights_ih) + bias_ih\n",
    "                hlayer_output = sigmoid(hlayer_logits)\n",
    "\n",
    "                final_logits = np.dot(hlayer_output, weights_ho) + bias_ho\n",
    "                final_output = sigmoid(final_logits)\n",
    "\n",
    "                tot_loss_valid += abs(final_output[0] - y_val[i])\n",
    "    cost_val.append(tot_loss_valid)\n",
    "    print(\"Done!\")\n",
    "    return cost, cost_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(X:np.ndarray, y:np.ndarray, k_folds:int = 4, epochs_per_fold:int = 250):\n",
    "        fold_size = len(X) // k_folds\n",
    "        losses = []\n",
    "        losses_val = []\n",
    "        for i in range(k_folds):\n",
    "            start, end = i * fold_size, (i + 1) * fold_size\n",
    "            X_test_fold, y_test_fold = X[start:end], y[start:end]\n",
    "            X_train_fold = np.concatenate([X[:start], X[end:]])\n",
    "            y_train_fold = np.concatenate([y[:start], y[end:]])\n",
    "            print(f\"Training Fold {i+1}\\n\")\n",
    "            loss_train, loss_val = train(X_train_fold, y_train_fold, X_test_fold, y_test_fold, epochs=epochs_per_fold)\n",
    "            losses.extend(loss_train)\n",
    "            losses_val.extend(loss_val)\n",
    "            print(\"\\n\"+10*\"----\"+\"\\n\")\n",
    "\n",
    "        return losses, losses_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Fold 1\n",
      "\n",
      "Done!\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Training Fold 2\n",
      "\n",
      "Done!\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Training Fold 3\n",
      "\n",
      "Done!\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Training Fold 4\n",
      "\n",
      "Done!\n",
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "losses, loss_val = k_fold_cross_validation(x, y, epochs_per_fold = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot( np.arange(0, len(losses)),losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot( np.arange(0, len(loss_val)),loss_val)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    inp = np.reshape(np.array([int(char) for char in x]), (1, input_size))\n",
    "    return sigmoid(np.dot(sigmoid(np.dot(inp, weights_ih)+ bias_ih), weights_ho)+bias_ho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in range(len(x)):\n",
    "    y_p = 1 if predict(x[i])>0.5 else 0\n",
    "    if y[i]!=y_p:\n",
    "        count+=1\n",
    "        print(x[i], ' - predicted: ', y_p)\n",
    "print('Accuracy: ', (len(x) - count)/len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
